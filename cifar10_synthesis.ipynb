{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "12h2Bc_KCmTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install python3.8 python3.8-distutils\n",
        "!update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n",
        "!update-alternatives --config python3\n",
        "!apt-get install python3-pip\n",
        "!python3 -m pip install --upgrade pip --user"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tbXLoKcP3EnX",
        "outputId": "02b4d20b-d305-4dfe-a71c-842a38093659"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib mailcap mime-support python3.8-lib2to3 python3.8-minimal\n",
            "Suggested packages:\n",
            "  python3.8-venv binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib mailcap mime-support python3.8 python3.8-distutils\n",
            "  python3.8-lib2to3 python3.8-minimal\n",
            "0 upgraded, 8 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 5,421 kB of archives.\n",
            "After this operation, 20.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n",
            "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-minimal amd64 3.8.19-1+jammy1 [794 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-minimal amd64 3.8.19-1+jammy1 [2,025 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.8-stdlib amd64 3.8.19-1+jammy1 [1,817 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8 amd64 3.8.19-1+jammy1 [439 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-lib2to3 all 3.8.19-1+jammy1 [126 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.8-distutils all 3.8.19-1+jammy1 [193 kB]\n",
            "Fetched 5,421 kB in 16s (329 kB/s)\n",
            "Selecting previously unselected package libpython3.8-minimal:amd64.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libpython3.8-minimal_3.8.19-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-minimal:amd64 (3.8.19-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-minimal.\n",
            "Preparing to unpack .../1-python3.8-minimal_3.8.19-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8-minimal (3.8.19-1+jammy1) ...\n",
            "Selecting previously unselected package mailcap.\n",
            "Preparing to unpack .../2-mailcap_3.70+nmu1ubuntu1_all.deb ...\n",
            "Unpacking mailcap (3.70+nmu1ubuntu1) ...\n",
            "Selecting previously unselected package mime-support.\n",
            "Preparing to unpack .../3-mime-support_3.66_all.deb ...\n",
            "Unpacking mime-support (3.66) ...\n",
            "Selecting previously unselected package libpython3.8-stdlib:amd64.\n",
            "Preparing to unpack .../4-libpython3.8-stdlib_3.8.19-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.8-stdlib:amd64 (3.8.19-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8.\n",
            "Preparing to unpack .../5-python3.8_3.8.19-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.8 (3.8.19-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-lib2to3.\n",
            "Preparing to unpack .../6-python3.8-lib2to3_3.8.19-1+jammy1_all.deb ...\n",
            "Unpacking python3.8-lib2to3 (3.8.19-1+jammy1) ...\n",
            "Selecting previously unselected package python3.8-distutils.\n",
            "Preparing to unpack .../7-python3.8-distutils_3.8.19-1+jammy1_all.deb ...\n",
            "Unpacking python3.8-distutils (3.8.19-1+jammy1) ...\n",
            "Setting up libpython3.8-minimal:amd64 (3.8.19-1+jammy1) ...\n",
            "Setting up python3.8-lib2to3 (3.8.19-1+jammy1) ...\n",
            "Setting up python3.8-minimal (3.8.19-1+jammy1) ...\n",
            "Setting up python3.8-distutils (3.8.19-1+jammy1) ...\n",
            "Setting up mailcap (3.70+nmu1ubuntu1) ...\n",
            "Setting up mime-support (3.66) ...\n",
            "Setting up libpython3.8-stdlib:amd64 (3.8.19-1+jammy1) ...\n",
            "Setting up python3.8 (3.8.19-1+jammy1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python3 (python3) in auto mode\n",
            "There is only one alternative in link group python3 (providing /usr/bin/python3): /usr/bin/python3.8\n",
            "Nothing to configure.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,677 kB of archives.\n",
            "After this operation, 8,967 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-setuptools all 59.6.0-1.2ubuntu0.22.04.1 [339 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.4 [1,305 kB]\n",
            "Fetched 1,677 kB in 1s (1,350 kB/s)\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "(Reading database ... 122708 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-setuptools_59.6.0-1.2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_22.0.2+dfsg-1ubuntu0.4_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.4) ...\n",
            "Setting up python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: pip in /usr/lib/python3/dist-packages (22.0.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "\u001b[33m  WARNING: The scripts pip, pip3, pip3.10 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed pip-24.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod -R 755 /usr/bin/python3.8"
      ],
      "metadata": {
        "id": "_40HU_r13_Jb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y make build-essential libssl-dev zlib1g-dev \\\n",
        "  libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \\\n",
        "  libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev \\\n",
        "  libsndfile-dev portaudio19-dev"
      ],
      "metadata": {
        "id": "lK5ifSLm9v_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bGt-bWh_3iTa",
        "outputId": "dec3a4f5-47e5-4626-a319-41ebb3ee83f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /root/.local/lib/python3.8/site-packages (24.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "c-VSmik5-W5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ..; rm -fr ai8x-training/; git clone --recursive https://github.com/analogdevicesinc/ai8x-training.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MtScJnN390O6",
        "outputId": "b9321e17-e5c9-4932-ee18-e15a08a6449e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai8x-training'...\n",
            "remote: Enumerating objects: 2815, done.\u001b[K\n",
            "remote: Counting objects: 100% (176/176), done.\u001b[K\n",
            "remote: Compressing objects: 100% (133/133), done.\u001b[K\n",
            "remote: Total 2815 (delta 86), reused 92 (delta 42), pack-reused 2639\u001b[K\n",
            "Receiving objects: 100% (2815/2815), 201.07 MiB | 37.67 MiB/s, done.\n",
            "Resolving deltas: 100% (1778/1778), done.\n",
            "Submodule 'distiller' (https://github.com/MaximIntegratedAI/distiller.git) registered for path 'distiller'\n",
            "Cloning into '/content/ai8x-training/distiller'...\n",
            "remote: Enumerating objects: 5915, done.        \n",
            "remote: Counting objects: 100% (5915/5915), done.        \n",
            "remote: Compressing objects: 100% (2157/2157), done.        \n",
            "remote: Total 5915 (delta 4289), reused 4714 (delta 3617), pack-reused 0        \n",
            "Receiving objects: 100% (5915/5915), 38.50 MiB | 27.49 MiB/s, done.\n",
            "Resolving deltas: 100% (4289/4289), done.\n",
            "Submodule path 'distiller': checked out '0477a66ef0ace09f5572f27c0178ea422ed9bf4e'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ai8x-training/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM-DAvIC90LN",
        "outputId": "6660f4f4-3a71-46e5-daf0-4e192d2ceab3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ai8x-training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv distiller/ distiller_sp/; cp  -r distiller_sp/distiller ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KGU2FCZ-xlX",
        "outputId": "04e159fc-f823-42a6-acde-065ccde66613"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'distiller/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source .venv/bin/activate;pip3 install -U pip wheel setuptools; pip3 install -r requirements.txt; pip3 install -r distiller_sp/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x0RyZyX390Hw",
        "outputId": "4167d7a1-f20b-4aad-fcd6-489715d5e9e4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: .venv/bin/activate: No such file or directory\n",
            "Requirement already satisfied: pip in /root/.local/lib/python3.8/site-packages (24.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (0.43.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (69.5.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mObtaining file:///content/ai8x-training/distiller (from -r requirements.txt (line 27))\n",
            "\u001b[31mERROR: file:///content/ai8x-training/distiller (from -r requirements.txt (line 27)) does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pillow>=7 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 1)) (10.3.0)\n",
            "Requirement already satisfied: numpy<1.23,>=1.22 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 3)) (1.8.1)\n",
            "Requirement already satisfied: torchvision==0.9.1 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 4)) (0.9.1)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 5)) (1.10.1)\n",
            "Requirement already satisfied: torchnet==0.0.4 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 6)) (0.0.4)\n",
            "Requirement already satisfied: pydot==1.4.1 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: tabulate==0.8.3 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 8)) (0.8.3)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 9)) (2.0.3)\n",
            "Requirement already satisfied: jupyter>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 10)) (1.0.0)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 11)) (3.7.5)\n",
            "Requirement already satisfied: qgrid==1.1.1 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 12)) (1.1.1)\n",
            "Requirement already satisfied: graphviz==0.10.1 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 13)) (0.10.1)\n",
            "Requirement already satisfied: ipywidgets==7.4.2 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 14)) (7.4.2)\n",
            "Requirement already satisfied: bqplot==0.11.5 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 15)) (0.11.5)\n",
            "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from -r distiller_sp/requirements.txt (line 16)) (5.4.1)\n",
            "Requirement already satisfied: pytest~=4.6.1 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 17)) (4.6.11)\n",
            "Requirement already satisfied: xlsxwriter>=1.2.2 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 18)) (3.2.0)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 19)) (0.7.4)\n",
            "Requirement already satisfied: scikit-learn<0.24,>=0.23.2 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 20)) (0.23.2)\n",
            "Requirement already satisfied: gym==0.12.5 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 21)) (0.12.5)\n",
            "Requirement already satisfied: tqdm==4.33.0 in /usr/local/lib/python3.8/dist-packages (from -r distiller_sp/requirements.txt (line 22)) (4.33.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1->-r distiller_sp/requirements.txt (line 3)) (4.11.0)\n",
            "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from torchnet==0.0.4->-r distiller_sp/requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: visdom in /usr/local/lib/python3.8/dist-packages (from torchnet==0.0.4->-r distiller_sp/requirements.txt (line 6)) (0.2.4)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.8/dist-packages (from pydot==1.4.1->-r distiller_sp/requirements.txt (line 7)) (3.1.2)\n",
            "Requirement already satisfied: notebook>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (7.1.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (6.29.4)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (5.14.3)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (5.10.4)\n",
            "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (3.4.2)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (8.12.3)\n",
            "Requirement already satisfied: traittypes>=0.0.6 in /usr/local/lib/python3.8/dist-packages (from bqplot==0.11.5->-r distiller_sp/requirements.txt (line 15)) (0.2.1)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.8/dist-packages (from pretrainedmodels==0.7.4->-r distiller_sp/requirements.txt (line 19)) (4.0.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.12.5->-r distiller_sp/requirements.txt (line 21)) (2.0.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.22.0->-r distiller_sp/requirements.txt (line 9)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.22.0->-r distiller_sp/requirements.txt (line 9)) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.22.0->-r distiller_sp/requirements.txt (line 9)) (2024.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.8/dist-packages (from jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (5.5.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.8/dist-packages (from jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (6.6.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.8/dist-packages (from jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (7.16.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.0->-r distiller_sp/requirements.txt (line 11)) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.0->-r distiller_sp/requirements.txt (line 11)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.0->-r distiller_sp/requirements.txt (line 11)) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.0->-r distiller_sp/requirements.txt (line 11)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.0->-r distiller_sp/requirements.txt (line 11)) (24.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.0->-r distiller_sp/requirements.txt (line 11)) (6.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest~=4.6.1->-r distiller_sp/requirements.txt (line 17)) (1.11.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from pytest~=4.6.1->-r distiller_sp/requirements.txt (line 17)) (23.2.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.8/dist-packages (from pytest~=4.6.1->-r distiller_sp/requirements.txt (line 17)) (1.4.1)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.8/dist-packages (from pytest~=4.6.1->-r distiller_sp/requirements.txt (line 17)) (0.13.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from pytest~=4.6.1->-r distiller_sp/requirements.txt (line 17)) (0.2.13)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytest~=4.6.1->-r distiller_sp/requirements.txt (line 17)) (10.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<0.24,>=0.23.2->-r distiller_sp/requirements.txt (line 20)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<0.24,>=0.23.2->-r distiller_sp/requirements.txt (line 20)) (3.5.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib~=3.0->-r distiller_sp/requirements.txt (line 11)) (3.18.1)\n",
            "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (0.2.2)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (1.8.1)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (8.6.1)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (5.7.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (5.9.8)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (26.0.3)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (6.4)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (0.19.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (2.18.0)\n",
            "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (0.6.3)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (4.9.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.8/dist-packages (from nbformat>=4.2.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat>=4.2.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (4.22.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (2.14.0)\n",
            "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (2.27.1)\n",
            "Requirement already satisfied: jupyterlab<4.2,>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (4.1.8)\n",
            "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (0.7.1)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (7.1.0)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (3.1.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (2.1.5)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from qtconsole->jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (2.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from visdom->torchnet==0.0.4->-r distiller_sp/requirements.txt (line 6)) (2.31.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.8/dist-packages (from visdom->torchnet==0.0.4->-r distiller_sp/requirements.txt (line 6)) (1.33)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.8/dist-packages (from visdom->torchnet==0.0.4->-r distiller_sp/requirements.txt (line 6)) (1.8.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from visdom->torchnet==0.0.4->-r distiller_sp/requirements.txt (line 6)) (3.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach!=5.0.0->nbconvert->jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (0.8.4)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (2023.12.1)\n",
            "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (1.3.10)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (0.18.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (4.2.1)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (4.3.0)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (23.1.0)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (0.10.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (0.5.3)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (7.7.0)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (0.20.0)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (0.18.1)\n",
            "Requirement already satisfied: async-lru>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from jupyterlab<4.2,>=4.1.1->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (2.0.4)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.8/dist-packages (from jupyterlab<4.2,>=4.1.1->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (0.27.0)\n",
            "Requirement already satisfied: jupyter-lsp>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from jupyterlab<4.2,>=4.1.1->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (2.2.5)\n",
            "Requirement already satisfied: tomli>=1.2.2 in /usr/local/lib/python3.8/dist-packages (from jupyterlab<4.2,>=4.1.1->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (2.0.1)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (2.15.0)\n",
            "Requirement already satisfied: json5>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (0.9.25)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->visdom->torchnet==0.0.4->-r distiller_sp/requirements.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->visdom->torchnet==0.0.4->-r distiller_sp/requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->visdom->torchnet==0.0.4->-r distiller_sp/requirements.txt (line 6)) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->visdom->torchnet==0.0.4->-r distiller_sp/requirements.txt (line 6)) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/dist-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r distiller_sp/requirements.txt (line 10)) (2.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.8/dist-packages (from jsonpatch->visdom->torchnet==0.0.4->-r distiller_sp/requirements.txt (line 6)) (2.4)\n",
            "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=4.0.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=4.0.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=4.0.0->ipywidgets==7.4.2->-r distiller_sp/requirements.txt (line 14)) (0.2.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.8/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (1.2.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.8/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (21.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.8/dist-packages (from httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.8/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (0.14.0)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.8/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (2.0.7)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.8/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (0.1.1)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.8/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.8/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (20.11.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.8/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.8/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (1.13)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (2.22)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.8/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.8/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->qgrid==1.1.1->-r distiller_sp/requirements.txt (line 12)) (2.9.0.20240316)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py --deterministic --epochs 50 --optimizer Adam --lr 0.01 --wd 0 --compress policies/schedule-cifar-nas.yaml --model ai85nascifarnet --dataset CIFAR10 --device MAX78000 --batch-size 100 --print-freq 100 --validation-split 0 --use-bias --qat-policy policies/qat_policy_late_cifar.yaml --confusion \"$@\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqjSnSaa90ES",
        "outputId": "ccd18d6a-9e9c-442d-8e39-707b51228a74"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuring device: MAX78000, simulate=False.\n",
            "Log file for this run: /content/ai8x-training/logs/2024.05.11-050006/2024.05.11-050006.log\n",
            "{'start_epoch': 210, 'weight_bits': 8, 'shift_quantile': 0.995}\n",
            "Optimizer Type: <class 'torch.optim.adam.Adam'>\n",
            "Optimizer Args: {'lr': 0.01, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\n",
            "Reading compression schedule from: policies/schedule-cifar-nas.yaml\n",
            "Dataset sizes:\n",
            "\ttraining=50000\n",
            "\tvalidation=10000\n",
            "\ttest=10000\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [0][  100/  500]    Overall Loss 1.853588    Objective Loss 1.853588                                        LR 0.010000    Time 0.045195    \n",
            "Epoch: [0][  200/  500]    Overall Loss 1.715597    Objective Loss 1.715597                                        LR 0.010000    Time 0.051285    \n",
            "Epoch: [0][  300/  500]    Overall Loss 1.615455    Objective Loss 1.615455                                        LR 0.010000    Time 0.047727    \n",
            "Epoch: [0][  400/  500]    Overall Loss 1.536737    Objective Loss 1.536737                                        LR 0.010000    Time 0.044805    \n",
            "Epoch: [0][  500/  500]    Overall Loss 1.466034    Objective Loss 1.466034    Top1 62.500000    Top5 96.000000    LR 0.010000    Time 0.042855    \n",
            "--- validate (epoch=0)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [0][  100/  100]    Loss 1.451096    Top1 52.450000    Top5 92.270000    \n",
            "==> Top1: 52.450    Top5: 92.270    Loss: 1.451\n",
            "\n",
            "==> Confusion:\n",
            "[[848  34  19   0  12   0  10   8  57  12]\n",
            " [ 86 811   2   2   7   0   3   2  46  41]\n",
            " [303  11 245  19 264   2  89  36  17  14]\n",
            " [158  33  80 149 183  19 166  56  96  60]\n",
            " [109  12  55   7 621   4  47 126  13   6]\n",
            " [111  21 107 203 170  81  76 141  55  35]\n",
            " [ 42   7  31   6 197   0 681   2  25   9]\n",
            " [102  30  24  15 162   1  16 616   4  30]\n",
            " [309  36   4   0  10   0   4   3 619  15]\n",
            " [147 209   0   0  13   0   6  22  29 574]]\n",
            "\n",
            "==> Best [Top1: 52.450   Top5: 92.270   Sparsity:0.00   Params: 301760 on epoch: 0]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [1][  100/  500]    Overall Loss 1.132428    Objective Loss 1.132428                                        LR 0.010000    Time 0.041627    \n",
            "Epoch: [1][  200/  500]    Overall Loss 1.103906    Objective Loss 1.103906                                        LR 0.010000    Time 0.039204    \n",
            "Epoch: [1][  300/  500]    Overall Loss 1.083717    Objective Loss 1.083717                                        LR 0.010000    Time 0.040810    \n",
            "Epoch: [1][  400/  500]    Overall Loss 1.058616    Objective Loss 1.058616                                        LR 0.010000    Time 0.044954    \n",
            "Epoch: [1][  500/  500]    Overall Loss 1.034772    Objective Loss 1.034772    Top1 67.000000    Top5 96.500000    LR 0.010000    Time 0.043122    \n",
            "--- validate (epoch=1)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [1][  100/  100]    Loss 1.178002    Top1 59.110000    Top5 96.350000    \n",
            "==> Top1: 59.110    Top5: 96.350    Loss: 1.178\n",
            "\n",
            "==> Confusion:\n",
            "[[795  51  70  12   0   5   1  40  24   2]\n",
            " [ 26 925   6  11   1   8   0  19   3   1]\n",
            " [109   3 509  39  74  91  24 145   5   1]\n",
            " [ 56   8  69 340  21 327  15 158   5   1]\n",
            " [ 33   3  95  60 430  63  18 294   4   0]\n",
            " [ 20   2  49  75  12 685   0 156   0   1]\n",
            " [ 29  10  66 157  74 111 508  44   1   0]\n",
            " [ 14   6  28   4   8  72   0 867   0   1]\n",
            " [226 108  24  28   4  14   2  17 573   4]\n",
            " [ 83 479   6  25   1  11   0 102  14 279]]\n",
            "\n",
            "==> Best [Top1: 59.110   Top5: 96.350   Sparsity:0.00   Params: 301760 on epoch: 1]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [2][  100/  500]    Overall Loss 0.903153    Objective Loss 0.903153                                        LR 0.010000    Time 0.049402    \n",
            "Epoch: [2][  200/  500]    Overall Loss 0.882543    Objective Loss 0.882543                                        LR 0.010000    Time 0.054234    \n",
            "Epoch: [2][  300/  500]    Overall Loss 0.871599    Objective Loss 0.871599                                        LR 0.010000    Time 0.048588    \n",
            "Epoch: [2][  400/  500]    Overall Loss 0.863631    Objective Loss 0.863631                                        LR 0.010000    Time 0.045587    \n",
            "Epoch: [2][  500/  500]    Overall Loss 0.848837    Objective Loss 0.848837    Top1 73.000000    Top5 98.500000    LR 0.010000    Time 0.044252    \n",
            "--- validate (epoch=2)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [2][  100/  100]    Loss 0.843076    Top1 71.100000    Top5 98.030000    \n",
            "==> Top1: 71.100    Top5: 98.030    Loss: 0.843\n",
            "\n",
            "==> Confusion:\n",
            "[[674  17  60  58  24  24   7  19  96  21]\n",
            " [ 13 836   2  25   3  15   9   6  40  51]\n",
            " [ 62   0 511 107  72 161  46  27   9   5]\n",
            " [ 11   1  29 661  18 221  27  12  14   6]\n",
            " [ 10   3  60 153 557  89  46  76   5   1]\n",
            " [  5   1   8 203  13 742   9  17   2   0]\n",
            " [  4   0  40 138  26  49 735   2   6   0]\n",
            " [  4   2  10  56  25 177   2 714   4   6]\n",
            " [ 46  16  14  25  11  11   2   1 867   7]\n",
            " [ 22  51   4  31   2  26   3   9  39 813]]\n",
            "\n",
            "==> Best [Top1: 71.100   Top5: 98.030   Sparsity:0.00   Params: 301760 on epoch: 2]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [3][  100/  500]    Overall Loss 0.766672    Objective Loss 0.766672                                        LR 0.010000    Time 0.041299    \n",
            "Epoch: [3][  200/  500]    Overall Loss 0.763862    Objective Loss 0.763862                                        LR 0.010000    Time 0.038911    \n",
            "Epoch: [3][  300/  500]    Overall Loss 0.754176    Objective Loss 0.754176                                        LR 0.010000    Time 0.039962    \n",
            "Epoch: [3][  400/  500]    Overall Loss 0.753927    Objective Loss 0.753927                                        LR 0.010000    Time 0.043937    \n",
            "Epoch: [3][  500/  500]    Overall Loss 0.746627    Objective Loss 0.746627    Top1 78.000000    Top5 97.000000    LR 0.010000    Time 0.042613    \n",
            "--- validate (epoch=3)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [3][  100/  100]    Loss 0.845064    Top1 70.910000    Top5 97.730000    \n",
            "==> Top1: 70.910    Top5: 97.730    Loss: 0.845\n",
            "\n",
            "==> Confusion:\n",
            "[[671   8  99   9  77   0  38  33  55  10]\n",
            " [  9 813   2   7   9   0  54  22  37  47]\n",
            " [ 45   0 507  57 153  17 183  24   8   6]\n",
            " [  8   1  48 576  84  34 185  44   9  11]\n",
            " [  9   1  17  39 793   4  73  64   0   0]\n",
            " [  6   0  43 297  75 408  96  68   4   3]\n",
            " [  2   0  14  34  38   1 908   1   1   1]\n",
            " [  5   1  10  53  68  23  31 804   0   5]\n",
            " [ 43   2  13  12  28   0  59   6 825  12]\n",
            " [ 20  38   3  20  20   0  17  50  46 786]]\n",
            "\n",
            "==> Best [Top1: 71.100   Top5: 98.030   Sparsity:0.00   Params: 301760 on epoch: 2]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [4][  100/  500]    Overall Loss 0.680670    Objective Loss 0.680670                                        LR 0.010000    Time 0.047820    \n",
            "Epoch: [4][  200/  500]    Overall Loss 0.689851    Objective Loss 0.689851                                        LR 0.010000    Time 0.053337    \n",
            "Epoch: [4][  300/  500]    Overall Loss 0.692239    Objective Loss 0.692239                                        LR 0.010000    Time 0.047970    \n",
            "Epoch: [4][  400/  500]    Overall Loss 0.685211    Objective Loss 0.685211                                        LR 0.010000    Time 0.045257    \n",
            "Epoch: [4][  500/  500]    Overall Loss 0.684234    Objective Loss 0.684234    Top1 76.500000    Top5 99.500000    LR 0.010000    Time 0.044295    \n",
            "--- validate (epoch=4)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [4][  100/  100]    Loss 0.695464    Top1 76.840000    Top5 98.290000    \n",
            "==> Top1: 76.840    Top5: 98.290    Loss: 0.695\n",
            "\n",
            "==> Confusion:\n",
            "[[882  23   9   4   6   0   4   5  51  16]\n",
            " [ 14 931   0   0   0   0   2   0  15  38]\n",
            " [141  13 579  37  72  40  65  17  22  14]\n",
            " [ 47  23  58 493  52 141  68  33  33  52]\n",
            " [ 33  12  58  23 771  15  30  32  10  16]\n",
            " [ 16   9  48 101  36 680  24  42  20  24]\n",
            " [ 20   7  35  33  31   8 829   2  21  14]\n",
            " [ 45  11  37  16  51  38   4 761   9  28]\n",
            " [ 70  24   1   0   2   1   1   2 889  10]\n",
            " [ 27  73   2   1   0   1   1   0  26 869]]\n",
            "\n",
            "==> Best [Top1: 76.840   Top5: 98.290   Sparsity:0.00   Params: 301760 on epoch: 4]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [5][  100/  500]    Overall Loss 0.654397    Objective Loss 0.654397                                        LR 0.010000    Time 0.065687    \n",
            "Epoch: [5][  200/  500]    Overall Loss 0.647197    Objective Loss 0.647197                                        LR 0.010000    Time 0.051011    \n",
            "Epoch: [5][  300/  500]    Overall Loss 0.642689    Objective Loss 0.642689                                        LR 0.010000    Time 0.053172    \n",
            "Epoch: [5][  400/  500]    Overall Loss 0.639324    Objective Loss 0.639324                                        LR 0.010000    Time 0.050910    \n",
            "Epoch: [5][  500/  500]    Overall Loss 0.636276    Objective Loss 0.636276    Top1 82.500000    Top5 98.500000    LR 0.010000    Time 0.047964    \n",
            "--- validate (epoch=5)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [5][  100/  100]    Loss 0.765961    Top1 74.270000    Top5 98.510000    \n",
            "==> Top1: 74.270    Top5: 98.510    Loss: 0.766\n",
            "\n",
            "==> Confusion:\n",
            "[[767   4  78  22   8   1  14   8  95   3]\n",
            " [ 24 814   3   3   0   9  14   1 107  25]\n",
            " [ 31   0 768  44  40  23  68  11  14   1]\n",
            " [ 14   2  88 617  38  47 133  21  37   3]\n",
            " [ 12   0 123  42 697   5  83  26  12   0]\n",
            " [  8   0  84 254  36 510  63  30  14   1]\n",
            " [  4   0  54  26   7   2 904   1   2   0]\n",
            " [ 17   1  36  43  45  19  25 779  27   8]\n",
            " [ 47   3  10  10   1   2  10   0 916   1]\n",
            " [ 47  26   7  13   1   0  12   1 238 655]]\n",
            "\n",
            "==> Best [Top1: 76.840   Top5: 98.290   Sparsity:0.00   Params: 301760 on epoch: 4]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [6][  100/  500]    Overall Loss 0.593269    Objective Loss 0.593269                                        LR 0.010000    Time 0.061529    \n",
            "Epoch: [6][  200/  500]    Overall Loss 0.590599    Objective Loss 0.590599                                        LR 0.010000    Time 0.054544    \n",
            "Epoch: [6][  300/  500]    Overall Loss 0.597988    Objective Loss 0.597988                                        LR 0.010000    Time 0.048335    \n",
            "Epoch: [6][  400/  500]    Overall Loss 0.599784    Objective Loss 0.599784                                        LR 0.010000    Time 0.045157    \n",
            "Epoch: [6][  500/  500]    Overall Loss 0.598347    Objective Loss 0.598347    Top1 79.500000    Top5 99.000000    LR 0.010000    Time 0.045929    \n",
            "--- validate (epoch=6)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [6][  100/  100]    Loss 0.824208    Top1 72.740000    Top5 98.340000    \n",
            "==> Top1: 72.740    Top5: 98.340    Loss: 0.824\n",
            "\n",
            "==> Confusion:\n",
            "[[901   5  18  27   1   0   0  24  16   8]\n",
            " [ 34 922   0   6   0   6   1   6   6  19]\n",
            " [110   0 640 110  19  68   7  42   1   3]\n",
            " [ 42   4  43 717  10 137   5  37   2   3]\n",
            " [ 38   3  81 150 501  63  12 149   3   0]\n",
            " [ 13   1  32 177   5 735   1  36   0   0]\n",
            " [ 20   2 114 280  14  61 485  14  10   0]\n",
            " [ 21   4   9  46   4  39   0 874   1   2]\n",
            " [200  24   4  38   2   5   0   4 711  12]\n",
            " [ 80  69   4  26   0   2   0  21  10 788]]\n",
            "\n",
            "==> Best [Top1: 76.840   Top5: 98.290   Sparsity:0.00   Params: 301760 on epoch: 4]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [7][  100/  500]    Overall Loss 0.590469    Objective Loss 0.590469                                        LR 0.010000    Time 0.041229    \n",
            "Epoch: [7][  200/  500]    Overall Loss 0.585343    Objective Loss 0.585343                                        LR 0.010000    Time 0.039117    \n",
            "Epoch: [7][  300/  500]    Overall Loss 0.576324    Objective Loss 0.576324                                        LR 0.010000    Time 0.045101    \n",
            "Epoch: [7][  400/  500]    Overall Loss 0.573726    Objective Loss 0.573726                                        LR 0.010000    Time 0.044456    \n",
            "Epoch: [7][  500/  500]    Overall Loss 0.567105    Objective Loss 0.567105    Top1 81.000000    Top5 99.500000    LR 0.010000    Time 0.042827    \n",
            "--- validate (epoch=7)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [7][  100/  100]    Loss 0.773653    Top1 75.390000    Top5 98.310000    \n",
            "==> Top1: 75.390    Top5: 98.310    Loss: 0.774\n",
            "\n",
            "==> Confusion:\n",
            "[[916  22  40   1   4   1   0   5   9   2]\n",
            " [ 15 979   0   0   0   0   0   0   1   5]\n",
            " [ 75   9 704  32  71  31  35  26   3  14]\n",
            " [101  20  61 460  84 122  63  49  16  24]\n",
            " [ 50   8  68  13 729   8  20  94   4   6]\n",
            " [ 34   7  57  69  42 690  24  62  10   5]\n",
            " [ 23  21  63  26  47  12 789   5   6   8]\n",
            " [ 57   7  21  14  18  15   7 848   5   8]\n",
            " [195  53   3   1   0   2   1   1 736   8]\n",
            " [ 95 199   6   2   1   0   0   0   9 688]]\n",
            "\n",
            "==> Best [Top1: 76.840   Top5: 98.290   Sparsity:0.00   Params: 301760 on epoch: 4]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [8][  100/  500]    Overall Loss 0.534755    Objective Loss 0.534755                                        LR 0.010000    Time 0.056998    \n",
            "Epoch: [8][  200/  500]    Overall Loss 0.537358    Objective Loss 0.537358                                        LR 0.010000    Time 0.051348    \n",
            "Epoch: [8][  300/  500]    Overall Loss 0.543358    Objective Loss 0.543358                                        LR 0.010000    Time 0.046581    \n",
            "Epoch: [8][  400/  500]    Overall Loss 0.542974    Objective Loss 0.542974                                        LR 0.010000    Time 0.043935    \n",
            "Epoch: [8][  500/  500]    Overall Loss 0.545743    Objective Loss 0.545743    Top1 83.000000    Top5 98.500000    LR 0.010000    Time 0.046028    \n",
            "--- validate (epoch=8)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [8][  100/  100]    Loss 0.613131    Top1 78.910000    Top5 98.930000    \n",
            "==> Top1: 78.910    Top5: 98.930    Loss: 0.613\n",
            "\n",
            "==> Confusion:\n",
            "[[934   4  10   2  14   1   0  15  17   3]\n",
            " [ 45 901   0   5   1   1   1   7  10  29]\n",
            " [112   3 687  24  70  32  20  46   3   3]\n",
            " [ 51   1  62 541  83 108  27 115   6   6]\n",
            " [ 31   2  45  16 801  19  14  72   0   0]\n",
            " [ 12   1  55  90  36 698   7 100   0   1]\n",
            " [ 29   1  49  63  38  14 775  21   5   5]\n",
            " [ 19   1  10   5  25  17   1 921   0   1]\n",
            " [129   3   5   5   5   2   0  11 834   6]\n",
            " [106  44   3   3   3   1   0  18  23 799]]\n",
            "\n",
            "==> Best [Top1: 78.910   Top5: 98.930   Sparsity:0.00   Params: 301760 on epoch: 8]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [9][  100/  500]    Overall Loss 0.509307    Objective Loss 0.509307                                        LR 0.010000    Time 0.042536    \n",
            "Epoch: [9][  200/  500]    Overall Loss 0.514153    Objective Loss 0.514153                                        LR 0.010000    Time 0.039197    \n",
            "Epoch: [9][  300/  500]    Overall Loss 0.520653    Objective Loss 0.520653                                        LR 0.010000    Time 0.045665    \n",
            "Epoch: [9][  400/  500]    Overall Loss 0.521785    Objective Loss 0.521785                                        LR 0.010000    Time 0.045256    \n",
            "Epoch: [9][  500/  500]    Overall Loss 0.521525    Objective Loss 0.521525    Top1 78.500000    Top5 99.000000    LR 0.010000    Time 0.043468    \n",
            "--- validate (epoch=9)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [9][  100/  100]    Loss 0.659386    Top1 78.060000    Top5 98.420000    \n",
            "==> Top1: 78.060    Top5: 98.420    Loss: 0.659\n",
            "\n",
            "==> Confusion:\n",
            "[[859  15   6  14   4   0   4   2  89   7]\n",
            " [ 11 945   0   2   0   0   2   0  21  19]\n",
            " [ 99  10 630  85  51  13  58  10  32  12]\n",
            " [ 20  14  21 781  37  25  30  10  37  25]\n",
            " [ 35   9  39  75 766   4  28  17  23   4]\n",
            " [ 14   9  38 332  31 516  16  21  15   8]\n",
            " [ 10   5  17  80  19   3 850   0  11   5]\n",
            " [ 97  20  15  82  47  12   8 685  15  19]\n",
            " [ 32  10   1   6   0   0   2   1 942   6]\n",
            " [ 34  90   1   4   1   0   1   0  37 832]]\n",
            "\n",
            "==> Best [Top1: 78.910   Top5: 98.930   Sparsity:0.00   Params: 301760 on epoch: 8]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [10][  100/  500]    Overall Loss 0.492843    Objective Loss 0.492843                                        LR 0.010000    Time 0.056305    \n",
            "Epoch: [10][  200/  500]    Overall Loss 0.499997    Objective Loss 0.499997                                        LR 0.010000    Time 0.053351    \n",
            "Epoch: [10][  300/  500]    Overall Loss 0.502832    Objective Loss 0.502832                                        LR 0.010000    Time 0.047512    \n",
            "Epoch: [10][  400/  500]    Overall Loss 0.501005    Objective Loss 0.501005                                        LR 0.010000    Time 0.044846    \n",
            "Epoch: [10][  500/  500]    Overall Loss 0.501190    Objective Loss 0.501190    Top1 83.000000    Top5 99.000000    LR 0.010000    Time 0.045453    \n",
            "--- validate (epoch=10)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [10][  100/  100]    Loss 0.627244    Top1 78.800000    Top5 98.990000    \n",
            "==> Top1: 78.800    Top5: 98.990    Loss: 0.627\n",
            "\n",
            "==> Confusion:\n",
            "[[854  24  12   8  28   9  13  21  14  17]\n",
            " [  3 950   1   1   0   4  10   2   1  28]\n",
            " [ 63   1 549  19 130 133  71  29   2   3]\n",
            " [  9   4  22 412  78 366  71  29   2   7]\n",
            " [  6   1   6  10 896  42  19  18   1   1]\n",
            " [  3   0   6  27  36 899  14  15   0   0]\n",
            " [  6   0   8  16  47  46 873   4   0   0]\n",
            " [  6   1   4   9  69 100   4 806   0   1]\n",
            " [ 95  44   4   6  10   9  12   5 783  32]\n",
            " [ 15  89   2   9   6   6   3   5   7 858]]\n",
            "\n",
            "==> Best [Top1: 78.910   Top5: 98.930   Sparsity:0.00   Params: 301760 on epoch: 8]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [11][  100/  500]    Overall Loss 0.483077    Objective Loss 0.483077                                        LR 0.010000    Time 0.067626    \n",
            "Epoch: [11][  200/  500]    Overall Loss 0.484404    Objective Loss 0.484404                                        LR 0.010000    Time 0.062240    \n",
            "Epoch: [11][  300/  500]    Overall Loss 0.476652    Objective Loss 0.476652                                        LR 0.010000    Time 0.056818    \n",
            "Epoch: [11][  400/  500]    Overall Loss 0.482366    Objective Loss 0.482366                                        LR 0.010000    Time 0.051662    \n",
            "Epoch: [11][  500/  500]    Overall Loss 0.484848    Objective Loss 0.484848    Top1 82.500000    Top5 99.500000    LR 0.010000    Time 0.048500    \n",
            "--- validate (epoch=11)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [11][  100/  100]    Loss 0.609412    Top1 78.900000    Top5 99.040000    \n",
            "==> Top1: 78.900    Top5: 99.040    Loss: 0.609\n",
            "\n",
            "==> Confusion:\n",
            "[[842  34  25   5   2   4   8  13  45  22]\n",
            " [  5 968   1   0   0   1   7   2   3  13]\n",
            " [ 68   8 678  35  25  82  53  39   6   6]\n",
            " [ 28  14  41 437  21 329  56  48  12  14]\n",
            " [ 21   3  47  31 633 141  47  67   8   2]\n",
            " [  9   6  13  40   9 878  11  32   1   1]\n",
            " [  8   7  24  34   8  31 876   9   3   0]\n",
            " [  9   8   9   3   3  85   3 877   0   3]\n",
            " [ 31  59   3   2   0   6   7   7 857  28]\n",
            " [ 11 119   1   0   1   7   5   3   9 844]]\n",
            "\n",
            "==> Best [Top1: 78.910   Top5: 98.930   Sparsity:0.00   Params: 301760 on epoch: 8]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [12][  100/  500]    Overall Loss 0.461719    Objective Loss 0.461719                                        LR 0.010000    Time 0.042791    \n",
            "Epoch: [12][  200/  500]    Overall Loss 0.465215    Objective Loss 0.465215                                        LR 0.010000    Time 0.039318    \n",
            "Epoch: [12][  300/  500]    Overall Loss 0.468442    Objective Loss 0.468442                                        LR 0.010000    Time 0.040190    \n",
            "Epoch: [12][  400/  500]    Overall Loss 0.464439    Objective Loss 0.464439                                        LR 0.010000    Time 0.045062    \n",
            "Epoch: [12][  500/  500]    Overall Loss 0.466823    Objective Loss 0.466823    Top1 80.500000    Top5 99.000000    LR 0.010000    Time 0.043235    \n",
            "--- validate (epoch=12)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [12][  100/  100]    Loss 0.569635    Top1 81.210000    Top5 99.000000    \n",
            "==> Top1: 81.210    Top5: 99.000    Loss: 0.570\n",
            "\n",
            "==> Confusion:\n",
            "[[788  24  48  12  25   0  15   7  44  37]\n",
            " [  1 943   0   2   1   0   4   0   3  46]\n",
            " [ 29   5 720  10  98  19  80  21   2  16]\n",
            " [  5  15  72 486  96 115 125  49   8  29]\n",
            " [  3   3  22  12 883   6  42  24   5   0]\n",
            " [  2   8  56  72  80 704  30  40   3   5]\n",
            " [  1   1  14   9  30   5 935   4   1   0]\n",
            " [  6   9  23   7  72  14  11 844   2  12]\n",
            " [ 27  23   5   3   1   0  16   2 886  37]\n",
            " [  7  42   2   6   2   0   3   0   6 932]]\n",
            "\n",
            "==> Best [Top1: 81.210   Top5: 99.000   Sparsity:0.00   Params: 301760 on epoch: 12]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [13][  100/  500]    Overall Loss 0.432081    Objective Loss 0.432081                                        LR 0.010000    Time 0.044262    \n",
            "Epoch: [13][  200/  500]    Overall Loss 0.432944    Objective Loss 0.432944                                        LR 0.010000    Time 0.051788    \n",
            "Epoch: [13][  300/  500]    Overall Loss 0.441921    Objective Loss 0.441921                                        LR 0.010000    Time 0.047662    \n",
            "Epoch: [13][  400/  500]    Overall Loss 0.449239    Objective Loss 0.449239                                        LR 0.010000    Time 0.044860    \n",
            "Epoch: [13][  500/  500]    Overall Loss 0.454408    Objective Loss 0.454408    Top1 82.500000    Top5 99.500000    LR 0.010000    Time 0.043128    \n",
            "--- validate (epoch=13)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [13][  100/  100]    Loss 0.663688    Top1 77.400000    Top5 98.860000    \n",
            "==> Top1: 77.400    Top5: 98.860    Loss: 0.664\n",
            "\n",
            "==> Confusion:\n",
            "[[841  15   3  77   1   0  15   0  12  36]\n",
            " [  6 915   0   6   0   1  21   0   1  50]\n",
            " [ 71   1 659 132  50  26  55   2   1   3]\n",
            " [  7   1  27 885  19  28  26   3   0   4]\n",
            " [ 16   1  30 145 740  10  45  12   0   1]\n",
            " [  4   0  14 342  14 598  20   7   0   1]\n",
            " [  4   0  33 114   4   5 840   0   0   0]\n",
            " [ 18   3  12 191  38  66  20 649   0   3]\n",
            " [104  34   3  88   3   0  15   1 710  42]\n",
            " [  6  35   3  40   0   0  11   0   2 903]]\n",
            "\n",
            "==> Best [Top1: 81.210   Top5: 99.000   Sparsity:0.00   Params: 301760 on epoch: 12]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [14][  100/  500]    Overall Loss 0.436345    Objective Loss 0.436345                                        LR 0.010000    Time 0.043036    \n",
            "Epoch: [14][  200/  500]    Overall Loss 0.437594    Objective Loss 0.437594                                        LR 0.010000    Time 0.039859    \n",
            "Epoch: [14][  300/  500]    Overall Loss 0.441035    Objective Loss 0.441035                                        LR 0.010000    Time 0.040979    \n",
            "Epoch: [14][  400/  500]    Overall Loss 0.438327    Objective Loss 0.438327                                        LR 0.010000    Time 0.045193    \n",
            "Epoch: [14][  500/  500]    Overall Loss 0.438632    Objective Loss 0.438632    Top1 84.500000    Top5 99.500000    LR 0.010000    Time 0.043339    \n",
            "--- validate (epoch=14)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [14][  100/  100]    Loss 0.636683    Top1 79.560000    Top5 98.880000    \n",
            "==> Top1: 79.560    Top5: 98.880    Loss: 0.637\n",
            "\n",
            "==> Confusion:\n",
            "[[878  26  13   4   7   1   0  23  31  17]\n",
            " [  3 971   0   2   0   1   2   2   4  15]\n",
            " [ 75   9 576  41  54  47  40 138   6  14]\n",
            " [ 23  10  10 574  33 135  33 157   8  17]\n",
            " [ 11  10  22  31 718  22  17 159   5   5]\n",
            " [  5   3   6  86  22 696  10 167   1   4]\n",
            " [ 11   3  13  40  21  13 853  42   3   1]\n",
            " [  6   6   3   7   3   5   0 963   0   7]\n",
            " [ 37  45   0   4   1   1   4   9 876  23]\n",
            " [ 21 106   1   1   0   0   3   7  10 851]]\n",
            "\n",
            "==> Best [Top1: 81.210   Top5: 99.000   Sparsity:0.00   Params: 301760 on epoch: 12]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [15][  100/  500]    Overall Loss 0.408078    Objective Loss 0.408078                                        LR 0.010000    Time 0.043506    \n",
            "Epoch: [15][  200/  500]    Overall Loss 0.421769    Objective Loss 0.421769                                        LR 0.010000    Time 0.051315    \n",
            "Epoch: [15][  300/  500]    Overall Loss 0.427850    Objective Loss 0.427850                                        LR 0.010000    Time 0.046828    \n",
            "Epoch: [15][  400/  500]    Overall Loss 0.428909    Objective Loss 0.428909                                        LR 0.010000    Time 0.044195    \n",
            "Epoch: [15][  500/  500]    Overall Loss 0.427870    Objective Loss 0.427870    Top1 87.500000    Top5 99.000000    LR 0.010000    Time 0.043199    \n",
            "--- validate (epoch=15)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [15][  100/  100]    Loss 0.560436    Top1 80.920000    Top5 99.160000    \n",
            "==> Top1: 80.920    Top5: 99.160    Loss: 0.560\n",
            "\n",
            "==> Confusion:\n",
            "[[923   1   6  18   9  15   5  16   6   1]\n",
            " [ 21 931   1   4   1  14   4   2   8  14]\n",
            " [ 66   0 669  57  37  90  51  26   2   2]\n",
            " [ 14   0  19 637  22 252  36  19   0   1]\n",
            " [ 24   1  33  62 749  67  29  33   2   0]\n",
            " [  3   1   6  62  10 900   3  14   1   0]\n",
            " [  9   2  13  51  12  33 876   4   0   0]\n",
            " [ 12   2   3  24  14  77   3 864   0   1]\n",
            " [133  13   4  24   5  24   9   5 778   5]\n",
            " [ 97  79   0  16   0  22   4   8   9 765]]\n",
            "\n",
            "==> Best [Top1: 81.210   Top5: 99.000   Sparsity:0.00   Params: 301760 on epoch: 12]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [16][  100/  500]    Overall Loss 0.414628    Objective Loss 0.414628                                        LR 0.010000    Time 0.043717    \n",
            "Epoch: [16][  200/  500]    Overall Loss 0.415142    Objective Loss 0.415142                                        LR 0.010000    Time 0.040163    \n",
            "Epoch: [16][  300/  500]    Overall Loss 0.413593    Objective Loss 0.413593                                        LR 0.010000    Time 0.041231    \n",
            "Epoch: [16][  400/  500]    Overall Loss 0.415634    Objective Loss 0.415634                                        LR 0.010000    Time 0.045571    \n",
            "Epoch: [16][  500/  500]    Overall Loss 0.416890    Objective Loss 0.416890    Top1 86.500000    Top5 100.000000    LR 0.010000    Time 0.043926    \n",
            "--- validate (epoch=16)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [16][  100/  100]    Loss 0.609670    Top1 80.300000    Top5 98.820000    \n",
            "==> Top1: 80.300    Top5: 98.820    Loss: 0.610\n",
            "\n",
            "==> Confusion:\n",
            "[[916   1  34   3   9   0   0   9  18  10]\n",
            " [ 39 826   0   1   0   0   1   1   9 123]\n",
            " [ 42   1 843  15  53   6   3  23   3  11]\n",
            " [ 41   2 135 564  72  53  13  79  10  31]\n",
            " [ 17   1  60   6 841   3   6  59   2   5]\n",
            " [ 17   0 123  92  46 612   6  87   4  13]\n",
            " [ 20   1 150  27  57   7 696  24   4  14]\n",
            " [ 17   0  29   7  21   4   0 911   0  11]\n",
            " [ 74   4   9   3   2   0   0   6 882  20]\n",
            " [ 41   4   5   0   3   0   0   0   8 939]]\n",
            "\n",
            "==> Best [Top1: 81.210   Top5: 99.000   Sparsity:0.00   Params: 301760 on epoch: 12]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [17][  100/  500]    Overall Loss 0.398056    Objective Loss 0.398056                                        LR 0.010000    Time 0.070982    \n",
            "Epoch: [17][  200/  500]    Overall Loss 0.399226    Objective Loss 0.399226                                        LR 0.010000    Time 0.062928    \n",
            "Epoch: [17][  300/  500]    Overall Loss 0.403302    Objective Loss 0.403302                                        LR 0.010000    Time 0.054253    \n",
            "Epoch: [17][  400/  500]    Overall Loss 0.407270    Objective Loss 0.407270                                        LR 0.010000    Time 0.049685    \n",
            "Epoch: [17][  500/  500]    Overall Loss 0.409916    Objective Loss 0.409916    Top1 85.500000    Top5 100.000000    LR 0.010000    Time 0.048800    \n",
            "--- validate (epoch=17)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [17][  100/  100]    Loss 0.621439    Top1 79.610000    Top5 98.820000    \n",
            "==> Top1: 79.610    Top5: 98.820    Loss: 0.621\n",
            "\n",
            "==> Confusion:\n",
            "[[862  59  27   7   7   0   0   0  25  13]\n",
            " [  3 989   0   0   1   0   0   0   1   6]\n",
            " [ 55  11 810  43  35   7  27   2   3   7]\n",
            " [ 24  28  73 779  40  14  18   7   5  12]\n",
            " [ 11  10  49  49 836   1  26  11   4   3]\n",
            " [ 13  21  89 336  43 479   8   9   0   2]\n",
            " [  7  13  68  75  15   1 815   0   1   5]\n",
            " [ 29  21  44  61  79   1   3 747   2  13]\n",
            " [ 36  94   6   6   3   0   0   0 838  17]\n",
            " [  8 172   2   2   1   0   0   0   9 806]]\n",
            "\n",
            "==> Best [Top1: 81.210   Top5: 99.000   Sparsity:0.00   Params: 301760 on epoch: 12]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [18][  100/  500]    Overall Loss 0.381725    Objective Loss 0.381725                                        LR 0.010000    Time 0.042587    \n",
            "Epoch: [18][  200/  500]    Overall Loss 0.383569    Objective Loss 0.383569                                        LR 0.010000    Time 0.039392    \n",
            "Epoch: [18][  300/  500]    Overall Loss 0.391117    Objective Loss 0.391117                                        LR 0.010000    Time 0.044983    \n",
            "Epoch: [18][  400/  500]    Overall Loss 0.389643    Objective Loss 0.389643                                        LR 0.010000    Time 0.044070    \n",
            "Epoch: [18][  500/  500]    Overall Loss 0.392197    Objective Loss 0.392197    Top1 88.500000    Top5 99.000000    LR 0.010000    Time 0.042810    \n",
            "--- validate (epoch=18)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [18][  100/  100]    Loss 0.552075    Top1 82.060000    Top5 99.110000    \n",
            "==> Top1: 82.060    Top5: 99.110    Loss: 0.552\n",
            "\n",
            "==> Confusion:\n",
            "[[937   6   6  11   2   0   0   0  28  10]\n",
            " [ 21 898   0   2   0   0   3   0  13  63]\n",
            " [ 94   0 654  91  47  33  59   7   7   8]\n",
            " [ 31   1  11 818  24  59  34   4   9   9]\n",
            " [ 32   1  13  75 797  32  28  12   7   3]\n",
            " [ 17   0   7 253  15 689  10   3   3   3]\n",
            " [  9   0   9  61  12   3 903   0   3   0]\n",
            " [ 52   2  10  92  39  99   5 680   5  16]\n",
            " [ 49   6   0   8   0   1   2   0 908  26]\n",
            " [ 32  24   0   6   0   0   3   0  13 922]]\n",
            "\n",
            "==> Best [Top1: 82.060   Top5: 99.110   Sparsity:0.00   Params: 301760 on epoch: 18]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [19][  100/  500]    Overall Loss 0.369030    Objective Loss 0.369030                                        LR 0.010000    Time 0.062817    \n",
            "Epoch: [19][  200/  500]    Overall Loss 0.376529    Objective Loss 0.376529                                        LR 0.010000    Time 0.053347    \n",
            "Epoch: [19][  300/  500]    Overall Loss 0.384791    Objective Loss 0.384791                                        LR 0.010000    Time 0.047697    \n",
            "Epoch: [19][  400/  500]    Overall Loss 0.384566    Objective Loss 0.384566                                        LR 0.010000    Time 0.045098    \n",
            "Epoch: [19][  500/  500]    Overall Loss 0.388373    Objective Loss 0.388373    Top1 89.000000    Top5 99.500000    LR 0.010000    Time 0.047060    \n",
            "--- validate (epoch=19)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [19][  100/  100]    Loss 0.540937    Top1 82.160000    Top5 99.150000    \n",
            "==> Top1: 82.160    Top5: 99.150    Loss: 0.541\n",
            "\n",
            "==> Confusion:\n",
            "[[962   7   6   3   3   0   0   1   9   9]\n",
            " [ 19 950   0   0   0   0   0   0   5  26]\n",
            " [ 99   2 734  40  50  15  43   9   2   6]\n",
            " [ 65  10  35 695  39  49  74   9   9  15]\n",
            " [ 38   2  29  33 841  12  35   6   4   0]\n",
            " [ 37   9  31 164  33 670  41   7   2   6]\n",
            " [ 29   5  13  17  12   5 913   0   4   2]\n",
            " [ 76  12  24  36  76  22   9 728   6  11]\n",
            " [119  16   0   2   2   0   2   0 842  17]\n",
            " [ 60  46   0   2   0   0   1   0  10 881]]\n",
            "\n",
            "==> Best [Top1: 82.160   Top5: 99.150   Sparsity:0.00   Params: 301760 on epoch: 19]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [20][  100/  500]    Overall Loss 0.374036    Objective Loss 0.374036                                        LR 0.010000    Time 0.043309    \n",
            "Epoch: [20][  200/  500]    Overall Loss 0.376580    Objective Loss 0.376580                                        LR 0.010000    Time 0.040645    \n",
            "Epoch: [20][  300/  500]    Overall Loss 0.376164    Objective Loss 0.376164                                        LR 0.010000    Time 0.046897    \n",
            "Epoch: [20][  400/  500]    Overall Loss 0.375946    Objective Loss 0.375946                                        LR 0.010000    Time 0.045362    \n",
            "Epoch: [20][  500/  500]    Overall Loss 0.377619    Objective Loss 0.377619    Top1 86.000000    Top5 99.500000    LR 0.010000    Time 0.043410    \n",
            "--- validate (epoch=20)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [20][  100/  100]    Loss 0.676875    Top1 78.750000    Top5 98.700000    \n",
            "==> Top1: 78.750    Top5: 98.700    Loss: 0.677\n",
            "\n",
            "==> Confusion:\n",
            "[[964  13   2   2   1   0   0   2   6  10]\n",
            " [  8 976   0   1   0   1   0   0   1  13]\n",
            " [170   7 623  38  18  90  34  15   0   5]\n",
            " [ 66  20  18 561  17 257  29  16   3  13]\n",
            " [ 52   6  26  60 686 100  29  36   3   2]\n",
            " [ 22  10   7  40  10 896   3  10   1   1]\n",
            " [ 26  13  19  41   6  65 819   4   3   4]\n",
            " [ 55  24   7  17   9 111   1 767   1   8]\n",
            " [190  48   1   3   1   4   0   2 739  12]\n",
            " [ 36 108   0   2   1   3   1   0   5 844]]\n",
            "\n",
            "==> Best [Top1: 82.160   Top5: 99.150   Sparsity:0.00   Params: 301760 on epoch: 19]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [21][  100/  500]    Overall Loss 0.361107    Objective Loss 0.361107                                        LR 0.010000    Time 0.058187    \n",
            "Epoch: [21][  200/  500]    Overall Loss 0.372067    Objective Loss 0.372067                                        LR 0.010000    Time 0.053413    \n",
            "Epoch: [21][  300/  500]    Overall Loss 0.368642    Objective Loss 0.368642                                        LR 0.010000    Time 0.047874    \n",
            "Epoch: [21][  400/  500]    Overall Loss 0.367897    Objective Loss 0.367897                                        LR 0.010000    Time 0.045327    \n",
            "Epoch: [21][  500/  500]    Overall Loss 0.370765    Objective Loss 0.370765    Top1 86.500000    Top5 99.500000    LR 0.010000    Time 0.046996    \n",
            "--- validate (epoch=21)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [21][  100/  100]    Loss 0.459949    Top1 84.860000    Top5 99.350000    \n",
            "==> Top1: 84.860    Top5: 99.350    Loss: 0.460\n",
            "\n",
            "==> Confusion:\n",
            "[[845  18  28  15  12   0   5   2  54  21]\n",
            " [  4 951   2   1   1   1   2   0   7  31]\n",
            " [ 39   0 817  27  36  17  51   6   2   5]\n",
            " [  6   7  66 710  38  82  71   8   6   6]\n",
            " [  4   1  29  32 849  16  52  13   3   1]\n",
            " [  6   1  40 131  27 763  21   5   3   3]\n",
            " [  3   1  23  16   7   3 943   0   3   1]\n",
            " [ 14   9  31  37  53  58  22 769   1   6]\n",
            " [ 19  16   4  10   2   1   6   0 927  15]\n",
            " [  7  56   2   4   1   0   7   0  11 912]]\n",
            "\n",
            "==> Best [Top1: 84.860   Top5: 99.350   Sparsity:0.00   Params: 301760 on epoch: 21]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [22][  100/  500]    Overall Loss 0.355497    Objective Loss 0.355497                                        LR 0.010000    Time 0.040369    \n",
            "Epoch: [22][  200/  500]    Overall Loss 0.358099    Objective Loss 0.358099                                        LR 0.010000    Time 0.038417    \n",
            "Epoch: [22][  300/  500]    Overall Loss 0.362648    Objective Loss 0.362648                                        LR 0.010000    Time 0.044672    \n",
            "Epoch: [22][  400/  500]    Overall Loss 0.362397    Objective Loss 0.362397                                        LR 0.010000    Time 0.044377    \n",
            "Epoch: [22][  500/  500]    Overall Loss 0.361686    Objective Loss 0.361686    Top1 88.500000    Top5 99.000000    LR 0.010000    Time 0.042587    \n",
            "--- validate (epoch=22)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [22][  100/  100]    Loss 0.532477    Top1 82.050000    Top5 99.380000    \n",
            "==> Top1: 82.050    Top5: 99.380    Loss: 0.532\n",
            "\n",
            "==> Confusion:\n",
            "[[818  24  14  36   3   3   2   2  79  19]\n",
            " [  3 964   0   3   1   3   1   1   8  16]\n",
            " [ 52   3 746  96  37  21  21  13   6   5]\n",
            " [ 12   3  26 855  23  54  11   4   8   4]\n",
            " [ 15   2  25 105 765  42  18  17  11   0]\n",
            " [  6   0  17 252  13 702   1   7   0   2]\n",
            " [  6   3  34 118  15  16 805   1   1   1]\n",
            " [ 11   6  12  97  19  78   2 769   1   5]\n",
            " [ 15  18   3  12   1   2   2   1 942   4]\n",
            " [  7 116   2  16   0   0   0   1  19 839]]\n",
            "\n",
            "==> Best [Top1: 84.860   Top5: 99.350   Sparsity:0.00   Params: 301760 on epoch: 21]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [23][  100/  500]    Overall Loss 0.341459    Objective Loss 0.341459                                        LR 0.010000    Time 0.062125    \n",
            "Epoch: [23][  200/  500]    Overall Loss 0.345305    Objective Loss 0.345305                                        LR 0.010000    Time 0.063381    \n",
            "Epoch: [23][  300/  500]    Overall Loss 0.349463    Objective Loss 0.349463                                        LR 0.010000    Time 0.055553    \n",
            "Epoch: [23][  400/  500]    Overall Loss 0.350603    Objective Loss 0.350603                                        LR 0.010000    Time 0.050765    \n",
            "Epoch: [23][  500/  500]    Overall Loss 0.356452    Objective Loss 0.356452    Top1 86.500000    Top5 99.000000    LR 0.010000    Time 0.049586    \n",
            "--- validate (epoch=23)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [23][  100/  100]    Loss 0.553277    Top1 82.380000    Top5 99.100000    \n",
            "==> Top1: 82.380    Top5: 99.100    Loss: 0.553\n",
            "\n",
            "==> Confusion:\n",
            "[[873   2  52  24  11   0   2   2  30   4]\n",
            " [ 29 858  11  16   1   6   4   2  39  34]\n",
            " [ 31   0 871  38  35  16   6   1   2   0]\n",
            " [  9   0  76 776  24  96  10   5   3   1]\n",
            " [  8   0  58  40 851  32   6   4   1   0]\n",
            " [  4   0  47 147  21 772   3   6   0   0]\n",
            " [  5   0  99  72  26  25 772   0   1   0]\n",
            " [ 10   0  61  49  87  80   0 711   1   1]\n",
            " [ 40   0   8  23   5   3   3   0 917   1]\n",
            " [ 48  17  21  31   8   6   4   5  23 837]]\n",
            "\n",
            "==> Best [Top1: 84.860   Top5: 99.350   Sparsity:0.00   Params: 301760 on epoch: 21]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [24][  100/  500]    Overall Loss 0.337253    Objective Loss 0.337253                                        LR 0.010000    Time 0.041403    \n",
            "Epoch: [24][  200/  500]    Overall Loss 0.344438    Objective Loss 0.344438                                        LR 0.010000    Time 0.039333    \n",
            "Epoch: [24][  300/  500]    Overall Loss 0.344088    Objective Loss 0.344088                                        LR 0.010000    Time 0.045096    \n",
            "Epoch: [24][  400/  500]    Overall Loss 0.343667    Objective Loss 0.343667                                        LR 0.010000    Time 0.043537    \n",
            "Epoch: [24][  500/  500]    Overall Loss 0.345358    Objective Loss 0.345358    Top1 85.500000    Top5 99.500000    LR 0.010000    Time 0.041905    \n",
            "--- validate (epoch=24)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [24][  100/  100]    Loss 0.467785    Top1 84.510000    Top5 99.350000    \n",
            "==> Top1: 84.510    Top5: 99.350    Loss: 0.468\n",
            "\n",
            "==> Confusion:\n",
            "[[789  25  39  30  19   4   3   3  54  34]\n",
            " [  2 935   2   2   3   1   4   0  10  41]\n",
            " [ 24   2 752  42  70  38  56  10   3   3]\n",
            " [  6   2  36 699  82  87  59   8   8  13]\n",
            " [  2   1  22  20 917  10  16   8   4   0]\n",
            " [  1   0  11 115  61 776  23  11   2   0]\n",
            " [  3   1  14  18  27   4 930   1   1   1]\n",
            " [  4   2  13  53  89  33   8 791   2   5]\n",
            " [ 23   9   5   5   1   0   7   1 936  13]\n",
            " [  3  41   4   7   2   0   4   0  13 926]]\n",
            "\n",
            "==> Best [Top1: 84.860   Top5: 99.350   Sparsity:0.00   Params: 301760 on epoch: 21]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [25][  100/  500]    Overall Loss 0.326198    Objective Loss 0.326198                                        LR 0.010000    Time 0.061043    \n",
            "Epoch: [25][  200/  500]    Overall Loss 0.334243    Objective Loss 0.334243                                        LR 0.010000    Time 0.053219    \n",
            "Epoch: [25][  300/  500]    Overall Loss 0.337067    Objective Loss 0.337067                                        LR 0.010000    Time 0.047337    \n",
            "Epoch: [25][  400/  500]    Overall Loss 0.340565    Objective Loss 0.340565                                        LR 0.010000    Time 0.044554    \n",
            "Epoch: [25][  500/  500]    Overall Loss 0.341741    Objective Loss 0.341741    Top1 88.000000    Top5 100.000000    LR 0.010000    Time 0.045860    \n",
            "--- validate (epoch=25)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [25][  100/  100]    Loss 0.418512    Top1 86.190000    Top5 99.430000    \n",
            "==> Top1: 86.190    Top5: 99.430    Loss: 0.419\n",
            "\n",
            "==> Confusion:\n",
            "[[887  18  30  10  15   4   2   2  18  14]\n",
            " [  4 957   2   2   1   1   1   0   6  26]\n",
            " [ 29   0 833  31  37  27  29  10   1   3]\n",
            " [ 13   2  68 711  40 128  18   9   3   8]\n",
            " [  7   0  42  32 846  23  20  26   4   0]\n",
            " [  4   0  26  82  26 834   9  16   1   2]\n",
            " [  4   1  50  55  12  13 863   2   0   0]\n",
            " [  8   3  26  18  18  36   1 887   0   3]\n",
            " [ 56  15   9   8   0   4   3   2 891  12]\n",
            " [ 13  53   3   6   1   1   3   3   7 910]]\n",
            "\n",
            "==> Best [Top1: 86.190   Top5: 99.430   Sparsity:0.00   Params: 301760 on epoch: 25]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [26][  100/  500]    Overall Loss 0.321697    Objective Loss 0.321697                                        LR 0.010000    Time 0.040905    \n",
            "Epoch: [26][  200/  500]    Overall Loss 0.325686    Objective Loss 0.325686                                        LR 0.010000    Time 0.039155    \n",
            "Epoch: [26][  300/  500]    Overall Loss 0.328454    Objective Loss 0.328454                                        LR 0.010000    Time 0.045188    \n",
            "Epoch: [26][  400/  500]    Overall Loss 0.332587    Objective Loss 0.332587                                        LR 0.010000    Time 0.044665    \n",
            "Epoch: [26][  500/  500]    Overall Loss 0.333737    Objective Loss 0.333737    Top1 89.500000    Top5 99.500000    LR 0.010000    Time 0.042720    \n",
            "--- validate (epoch=26)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [26][  100/  100]    Loss 0.587464    Top1 82.270000    Top5 99.150000    \n",
            "==> Top1: 82.270    Top5: 99.150    Loss: 0.587\n",
            "\n",
            "==> Confusion:\n",
            "[[900   5  11   7  17   0  10   3  46   1]\n",
            " [ 14 933   0   0   4   1  16   4  21   7]\n",
            " [ 62   1 700  13  63  36  97  24   3   1]\n",
            " [ 24   0  30 498  80 156 162  35  14   1]\n",
            " [  6   0  14   2 877  11  67  20   3   0]\n",
            " [ 11   1  12  44  53 792  50  32   5   0]\n",
            " [  5   1  10   5  12   4 959   3   1   0]\n",
            " [ 14   2   6   6  47  18   3 901   2   1]\n",
            " [ 35   7   2   3   4   0  12   1 936   0]\n",
            " [ 67 107   0   0   3   0  22   9  61 731]]\n",
            "\n",
            "==> Best [Top1: 86.190   Top5: 99.430   Sparsity:0.00   Params: 301760 on epoch: 25]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [27][  100/  500]    Overall Loss 0.320694    Objective Loss 0.320694                                        LR 0.010000    Time 0.058083    \n",
            "Epoch: [27][  200/  500]    Overall Loss 0.324782    Objective Loss 0.324782                                        LR 0.010000    Time 0.053392    \n",
            "Epoch: [27][  300/  500]    Overall Loss 0.323255    Objective Loss 0.323255                                        LR 0.010000    Time 0.047778    \n",
            "Epoch: [27][  400/  500]    Overall Loss 0.325488    Objective Loss 0.325488                                        LR 0.010000    Time 0.044833    \n",
            "Epoch: [27][  500/  500]    Overall Loss 0.328685    Objective Loss 0.328685    Top1 91.500000    Top5 100.000000    LR 0.010000    Time 0.045535    \n",
            "--- validate (epoch=27)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [27][  100/  100]    Loss 0.458453    Top1 85.580000    Top5 99.310000    \n",
            "==> Top1: 85.580    Top5: 99.310    Loss: 0.458\n",
            "\n",
            "==> Confusion:\n",
            "[[941   9  13   3   5   0   0   3  20   6]\n",
            " [  8 950   0   0   0   0   0   3   4  35]\n",
            " [ 80   2 763  17  30  28  43  27   3   7]\n",
            " [ 46   5  46 607  49 133  39  46  13  16]\n",
            " [ 13   3  37  16 856  11  21  39   2   2]\n",
            " [ 17   1  21  58  27 817   9  42   2   6]\n",
            " [ 10   3  30  28  13  12 898   3   2   1]\n",
            " [ 18   2  12   8  14  14   2 927   2   1]\n",
            " [ 64  29   0   2   1   1   1   3 892   7]\n",
            " [ 42  35   0   1   1   0   1   2  11 907]]\n",
            "\n",
            "==> Best [Top1: 86.190   Top5: 99.430   Sparsity:0.00   Params: 301760 on epoch: 25]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [28][  100/  500]    Overall Loss 0.304851    Objective Loss 0.304851                                        LR 0.010000    Time 0.040813    \n",
            "Epoch: [28][  200/  500]    Overall Loss 0.318855    Objective Loss 0.318855                                        LR 0.010000    Time 0.038455    \n",
            "Epoch: [28][  300/  500]    Overall Loss 0.323674    Objective Loss 0.323674                                        LR 0.010000    Time 0.044272    \n",
            "Epoch: [28][  400/  500]    Overall Loss 0.324374    Objective Loss 0.324374                                        LR 0.010000    Time 0.044197    \n",
            "Epoch: [28][  500/  500]    Overall Loss 0.324877    Objective Loss 0.324877    Top1 86.500000    Top5 100.000000    LR 0.010000    Time 0.042449    \n",
            "--- validate (epoch=28)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [28][  100/  100]    Loss 0.552652    Top1 83.330000    Top5 99.090000    \n",
            "==> Top1: 83.330    Top5: 99.090    Loss: 0.553\n",
            "\n",
            "==> Confusion:\n",
            "[[839   5  23   7   1   0   0   2 109  14]\n",
            " [  8 880   0   1   0   1   0   0  43  67]\n",
            " [ 49   0 802  14  32  11  58  13  16   5]\n",
            " [ 30   3  82 668  35  56  50  25  37  14]\n",
            " [ 20   1  34  40 786  16  53  21  25   4]\n",
            " [ 14   2  66 124  20 702  24  30  11   7]\n",
            " [  7   2  30  13   5   1 925   7   9   1]\n",
            " [ 32   6  25  21  16  15   6 851  17  11]\n",
            " [ 18   2   2   3   0   0   3   0 970   2]\n",
            " [  8  19   3   2   0   0   2   0  56 910]]\n",
            "\n",
            "==> Best [Top1: 86.190   Top5: 99.430   Sparsity:0.00   Params: 301760 on epoch: 25]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [29][  100/  500]    Overall Loss 0.318461    Objective Loss 0.318461                                        LR 0.010000    Time 0.058426    \n",
            "Epoch: [29][  200/  500]    Overall Loss 0.318478    Objective Loss 0.318478                                        LR 0.010000    Time 0.059396    \n",
            "Epoch: [29][  300/  500]    Overall Loss 0.318640    Objective Loss 0.318640                                        LR 0.010000    Time 0.056182    \n",
            "Epoch: [29][  400/  500]    Overall Loss 0.316102    Objective Loss 0.316102                                        LR 0.010000    Time 0.053386    \n",
            "Epoch: [29][  500/  500]    Overall Loss 0.316004    Objective Loss 0.316004    Top1 85.500000    Top5 98.500000    LR 0.010000    Time 0.053713    \n",
            "--- validate (epoch=29)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [29][  100/  100]    Loss 0.555833    Top1 82.540000    Top5 99.170000    \n",
            "==> Top1: 82.540    Top5: 99.170    Loss: 0.556\n",
            "\n",
            "==> Confusion:\n",
            "[[907  33  17   0   2   3   0  11  20   7]\n",
            " [  5 985   0   1   0   0   0   0   2   7]\n",
            " [ 56   7 775   7  26  46  30  41   4   8]\n",
            " [ 42  18  52 388  29 330  45  68   7  21]\n",
            " [ 21   6  45  12 787  39  13  74   3   0]\n",
            " [ 11   4  16  17  15 883   8  43   0   3]\n",
            " [  9  11  28  20  11  19 884  14   3   1]\n",
            " [ 12   5   4   1   7  30   1 936   0   4]\n",
            " [ 51  51   1   2   1   2   0   2 880  10]\n",
            " [ 16 144   1   0   0   1   1   3   5 829]]\n",
            "\n",
            "==> Best [Top1: 86.190   Top5: 99.430   Sparsity:0.00   Params: 301760 on epoch: 25]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [30][  100/  500]    Overall Loss 0.310414    Objective Loss 0.310414                                        LR 0.010000    Time 0.041158    \n",
            "Epoch: [30][  200/  500]    Overall Loss 0.311784    Objective Loss 0.311784                                        LR 0.010000    Time 0.041063    \n",
            "Epoch: [30][  300/  500]    Overall Loss 0.313119    Objective Loss 0.313119                                        LR 0.010000    Time 0.046955    \n",
            "Epoch: [30][  400/  500]    Overall Loss 0.315544    Objective Loss 0.315544                                        LR 0.010000    Time 0.044309    \n",
            "Epoch: [30][  500/  500]    Overall Loss 0.316239    Objective Loss 0.316239    Top1 89.500000    Top5 100.000000    LR 0.010000    Time 0.042613    \n",
            "--- validate (epoch=30)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [30][  100/  100]    Loss 0.508561    Top1 83.640000    Top5 99.330000    \n",
            "==> Top1: 83.640    Top5: 99.330    Loss: 0.509\n",
            "\n",
            "==> Confusion:\n",
            "[[819   4  65  10  15   0  16   4  42  25]\n",
            " [  5 926   1   2   1   4  14   0   9  38]\n",
            " [ 21   1 831  11  31  26  72   2   3   2]\n",
            " [ 10   0  85 569  45 178  97  10   3   3]\n",
            " [  2   1  52  13 831  29  62   7   3   0]\n",
            " [  4   0  37  57  21 856  20   5   0   0]\n",
            " [  3   0  40   7   3   5 941   0   1   0]\n",
            " [  9   1  35  13  37  93  16 782   6   8]\n",
            " [ 28   8   8   3   2   2  21   0 923   5]\n",
            " [ 11  43   5   7   0   4  18   1  25 886]]\n",
            "\n",
            "==> Best [Top1: 86.190   Top5: 99.430   Sparsity:0.00   Params: 301760 on epoch: 25]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [31][  100/  500]    Overall Loss 0.306301    Objective Loss 0.306301                                        LR 0.010000    Time 0.064749    \n",
            "Epoch: [31][  200/  500]    Overall Loss 0.303253    Objective Loss 0.303253                                        LR 0.010000    Time 0.051556    \n",
            "Epoch: [31][  300/  500]    Overall Loss 0.306605    Objective Loss 0.306605                                        LR 0.010000    Time 0.046223    \n",
            "Epoch: [31][  400/  500]    Overall Loss 0.309930    Objective Loss 0.309930                                        LR 0.010000    Time 0.044253    \n",
            "Epoch: [31][  500/  500]    Overall Loss 0.310106    Objective Loss 0.310106    Top1 90.000000    Top5 100.000000    LR 0.010000    Time 0.046667    \n",
            "--- validate (epoch=31)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [31][  100/  100]    Loss 0.455429    Top1 85.610000    Top5 99.200000    \n",
            "==> Top1: 85.610    Top5: 99.200    Loss: 0.455\n",
            "\n",
            "==> Confusion:\n",
            "[[877   7  39   5   9   0   1   2  41  19]\n",
            " [  7 930   0   2   0   0   0   0   6  55]\n",
            " [ 31   1 846  31  24  11  28  12   8   8]\n",
            " [ 22   3  73 730  38  64  21  20   9  20]\n",
            " [ 11   3  53  35 817  18  18  41   4   0]\n",
            " [ 16   0  40 106  25 751   4  36   6  16]\n",
            " [  8   2  48  61  15   8 841   5   4   8]\n",
            " [ 18   1  28  23  20  10   0 887   1  12]\n",
            " [ 32   8   8   1   0   0   0   1 935  15]\n",
            " [ 11  16   2   3   1   0   1   0  19 947]]\n",
            "\n",
            "==> Best [Top1: 86.190   Top5: 99.430   Sparsity:0.00   Params: 301760 on epoch: 25]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [32][  100/  500]    Overall Loss 0.290720    Objective Loss 0.290720                                        LR 0.010000    Time 0.041413    \n",
            "Epoch: [32][  200/  500]    Overall Loss 0.292576    Objective Loss 0.292576                                        LR 0.010000    Time 0.038669    \n",
            "Epoch: [32][  300/  500]    Overall Loss 0.294331    Objective Loss 0.294331                                        LR 0.010000    Time 0.045339    \n",
            "Epoch: [32][  400/  500]    Overall Loss 0.297265    Objective Loss 0.297265                                        LR 0.010000    Time 0.044849    \n",
            "Epoch: [32][  500/  500]    Overall Loss 0.298103    Objective Loss 0.298103    Top1 91.000000    Top5 99.500000    LR 0.010000    Time 0.043247    \n",
            "--- validate (epoch=32)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [32][  100/  100]    Loss 0.478581    Top1 84.890000    Top5 99.180000    \n",
            "==> Top1: 84.890    Top5: 99.180    Loss: 0.479\n",
            "\n",
            "==> Confusion:\n",
            "[[853   3  77  17   8   0  15  11  13   3]\n",
            " [ 11 938   9   9   0   1  11   3   6  12]\n",
            " [ 22   0 850  22  33  12  45  15   0   1]\n",
            " [  8   1  50 772  30  40  79  17   2   1]\n",
            " [  4   0  27  39 844  10  53  22   1   0]\n",
            " [  4   0  42 178  28 673  37  37   0   1]\n",
            " [  2   0  23  21   5   1 946   2   0   0]\n",
            " [  5   0  19  23  25  15   6 906   0   1]\n",
            " [ 53   7  17  14   5   0  26   5 871   2]\n",
            " [ 31  63  15  15   0   1  17  12  10 836]]\n",
            "\n",
            "==> Best [Top1: 86.190   Top5: 99.430   Sparsity:0.00   Params: 301760 on epoch: 25]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [33][  100/  500]    Overall Loss 0.293105    Objective Loss 0.293105                                        LR 0.010000    Time 0.059542    \n",
            "Epoch: [33][  200/  500]    Overall Loss 0.286436    Objective Loss 0.286436                                        LR 0.010000    Time 0.052927    \n",
            "Epoch: [33][  300/  500]    Overall Loss 0.290544    Objective Loss 0.290544                                        LR 0.010000    Time 0.047545    \n",
            "Epoch: [33][  400/  500]    Overall Loss 0.292705    Objective Loss 0.292705                                        LR 0.010000    Time 0.044986    \n",
            "Epoch: [33][  500/  500]    Overall Loss 0.295759    Objective Loss 0.295759    Top1 87.000000    Top5 99.500000    LR 0.010000    Time 0.046681    \n",
            "--- validate (epoch=33)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [33][  100/  100]    Loss 0.434543    Top1 86.280000    Top5 99.410000    \n",
            "==> Top1: 86.280    Top5: 99.410    Loss: 0.435\n",
            "\n",
            "==> Confusion:\n",
            "[[910  10  15   1   6   0   2   3  47   6]\n",
            " [  4 962   0   0   1   0   0   0  11  22]\n",
            " [ 50   3 790  12  65  15  41  15   5   4]\n",
            " [ 28   5  39 684  72  50  75  24  16   7]\n",
            " [ 10   1  22   7 908  12  14  17   8   1]\n",
            " [ 16   2  34 127  51 717  24  24   3   2]\n",
            " [  7   4  22   8  25   1 927   4   2   0]\n",
            " [ 14   2   8  16  36  21   2 895   3   3]\n",
            " [ 25  14   2   0   1   0   4   1 952   1]\n",
            " [ 34  48   0   0   1   1   4   4  25 883]]\n",
            "\n",
            "==> Best [Top1: 86.280   Top5: 99.410   Sparsity:0.00   Params: 301760 on epoch: 33]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [34][  100/  500]    Overall Loss 0.277241    Objective Loss 0.277241                                        LR 0.010000    Time 0.043999    \n",
            "Epoch: [34][  200/  500]    Overall Loss 0.283200    Objective Loss 0.283200                                        LR 0.010000    Time 0.040633    \n",
            "Epoch: [34][  300/  500]    Overall Loss 0.286693    Objective Loss 0.286693                                        LR 0.010000    Time 0.046560    \n",
            "Epoch: [34][  400/  500]    Overall Loss 0.288240    Objective Loss 0.288240                                        LR 0.010000    Time 0.045249    \n",
            "Epoch: [34][  500/  500]    Overall Loss 0.290127    Objective Loss 0.290127    Top1 91.000000    Top5 99.500000    LR 0.010000    Time 0.043508    \n",
            "--- validate (epoch=34)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [34][  100/  100]    Loss 0.440587    Top1 86.070000    Top5 99.510000    \n",
            "==> Top1: 86.070    Top5: 99.510    Loss: 0.441\n",
            "\n",
            "==> Confusion:\n",
            "[[831   2  37  17  17   1   6   4  77   8]\n",
            " [  9 836   2   2   0   0   1   1  49 100]\n",
            " [ 24   0 824  38  49  15  31   8   6   5]\n",
            " [  8   0  38 756  48  90  31  12   5  12]\n",
            " [  2   0  27  34 888  16  16  15   2   0]\n",
            " [  5   0  18 114  33 807   5  16   1   1]\n",
            " [  4   0  24  49  18  15 881   4   4   1]\n",
            " [  4   1  23  32  31  15   1 885   5   3]\n",
            " [ 19   1   1   5   2   1   1   4 962   4]\n",
            " [ 10   8   4   6   1   0   1   0  33 937]]\n",
            "\n",
            "==> Best [Top1: 86.280   Top5: 99.410   Sparsity:0.00   Params: 301760 on epoch: 33]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [35][  100/  500]    Overall Loss 0.258017    Objective Loss 0.258017                                        LR 0.010000    Time 0.057874    \n",
            "Epoch: [35][  200/  500]    Overall Loss 0.271775    Objective Loss 0.271775                                        LR 0.010000    Time 0.057427    \n",
            "Epoch: [35][  300/  500]    Overall Loss 0.279878    Objective Loss 0.279878                                        LR 0.010000    Time 0.055989    \n",
            "Epoch: [35][  400/  500]    Overall Loss 0.284004    Objective Loss 0.284004                                        LR 0.010000    Time 0.054296    \n",
            "Epoch: [35][  500/  500]    Overall Loss 0.285483    Objective Loss 0.285483    Top1 87.500000    Top5 100.000000    LR 0.010000    Time 0.054191    \n",
            "--- validate (epoch=35)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [35][  100/  100]    Loss 0.435924    Top1 86.440000    Top5 99.460000    \n",
            "==> Top1: 86.440    Top5: 99.460    Loss: 0.436\n",
            "\n",
            "==> Confusion:\n",
            "[[860   3  49  21   7   1   4   3  40  12]\n",
            " [ 13 871   5   4   0   0   1   1  21  84]\n",
            " [ 20   0 882  15  17  25  22  12   2   5]\n",
            " [  7   1  71 739  30  95  35  10   3   9]\n",
            " [  4   0  80  38 829  14   7  26   1   1]\n",
            " [  5   0  46  93  27 797   8  21   1   2]\n",
            " [  5   1  47  38  12  10 884   2   1   0]\n",
            " [  6   0  27  20  18  18   0 905   2   4]\n",
            " [ 33   4   9   7   0   2   4   4 932   5]\n",
            " [ 15  11   7   8   0   0   1   0  13 945]]\n",
            "\n",
            "==> Best [Top1: 86.440   Top5: 99.460   Sparsity:0.00   Params: 301760 on epoch: 35]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [36][  100/  500]    Overall Loss 0.262296    Objective Loss 0.262296                                        LR 0.010000    Time 0.040698    \n",
            "Epoch: [36][  200/  500]    Overall Loss 0.272206    Objective Loss 0.272206                                        LR 0.010000    Time 0.042667    \n",
            "Epoch: [36][  300/  500]    Overall Loss 0.276508    Objective Loss 0.276508                                        LR 0.010000    Time 0.047115    \n",
            "Epoch: [36][  400/  500]    Overall Loss 0.279509    Objective Loss 0.279509                                        LR 0.010000    Time 0.044408    \n",
            "Epoch: [36][  500/  500]    Overall Loss 0.282975    Objective Loss 0.282975    Top1 90.000000    Top5 99.500000    LR 0.010000    Time 0.042859    \n",
            "--- validate (epoch=36)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [36][  100/  100]    Loss 0.513619    Top1 84.290000    Top5 99.310000    \n",
            "==> Top1: 84.290    Top5: 99.310    Loss: 0.514\n",
            "\n",
            "==> Confusion:\n",
            "[[946  13  18   0   7   0   0   0  15   1]\n",
            " [  9 965   0   0   1   1   0   1  13  10]\n",
            " [ 61   1 851  18  29  11  13  10   3   3]\n",
            " [ 63  10 100 627  58  57  30  33  12  10]\n",
            " [ 25   1  47   3 895   5   7  13   4   0]\n",
            " [ 30   3  57  94  38 712   9  52   1   4]\n",
            " [ 28   6  77  12  15   3 851   3   4   1]\n",
            " [ 32   2  26   7  48  11   0 873   0   1]\n",
            " [ 65  20   3   4   1   0   0   0 906   1]\n",
            " [ 72 103   2   2   2   0   0   2  14 803]]\n",
            "\n",
            "==> Best [Top1: 86.440   Top5: 99.460   Sparsity:0.00   Params: 301760 on epoch: 35]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [37][  100/  500]    Overall Loss 0.261077    Objective Loss 0.261077                                        LR 0.010000    Time 0.067874    \n",
            "Epoch: [37][  200/  500]    Overall Loss 0.268947    Objective Loss 0.268947                                        LR 0.010000    Time 0.051658    \n",
            "Epoch: [37][  300/  500]    Overall Loss 0.276675    Objective Loss 0.276675                                        LR 0.010000    Time 0.046705    \n",
            "Epoch: [37][  400/  500]    Overall Loss 0.276686    Objective Loss 0.276686                                        LR 0.010000    Time 0.045375    \n",
            "Epoch: [37][  500/  500]    Overall Loss 0.276603    Objective Loss 0.276603    Top1 92.000000    Top5 99.500000    LR 0.010000    Time 0.047888    \n",
            "--- validate (epoch=37)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [37][  100/  100]    Loss 0.427390    Top1 86.750000    Top5 99.330000    \n",
            "==> Top1: 86.750    Top5: 99.330    Loss: 0.427\n",
            "\n",
            "==> Confusion:\n",
            "[[901  12   5  13   5   0   0   1  42  21]\n",
            " [  6 952   0   2   0   0   0   0  10  30]\n",
            " [ 67   0 751  41  47  28  42   9   8   7]\n",
            " [ 24   6  16 780  36  66  48   4  12   8]\n",
            " [ 13   1  14  40 863  19  32  16   2   0]\n",
            " [ 13   1  17 137  22 782  14   9   2   3]\n",
            " [  9   3  15  27  13   4 923   1   2   3]\n",
            " [ 15   2   5  38  36  41   3 849   5   6]\n",
            " [ 25  10   1   3   1   1   3   1 945  10]\n",
            " [ 10  35   0   7   1   0   5   0  13 929]]\n",
            "\n",
            "==> Best [Top1: 86.750   Top5: 99.330   Sparsity:0.00   Params: 301760 on epoch: 37]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [38][  100/  500]    Overall Loss 0.262070    Objective Loss 0.262070                                        LR 0.010000    Time 0.040856    \n",
            "Epoch: [38][  200/  500]    Overall Loss 0.269926    Objective Loss 0.269926                                        LR 0.010000    Time 0.039638    \n",
            "Epoch: [38][  300/  500]    Overall Loss 0.273614    Objective Loss 0.273614                                        LR 0.010000    Time 0.045647    \n",
            "Epoch: [38][  400/  500]    Overall Loss 0.275526    Objective Loss 0.275526                                        LR 0.010000    Time 0.043308    \n",
            "Epoch: [38][  500/  500]    Overall Loss 0.275604    Objective Loss 0.275604    Top1 89.000000    Top5 99.500000    LR 0.010000    Time 0.041721    \n",
            "--- validate (epoch=38)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [38][  100/  100]    Loss 0.495476    Top1 85.700000    Top5 99.310000    \n",
            "==> Top1: 85.700    Top5: 99.310    Loss: 0.495\n",
            "\n",
            "==> Confusion:\n",
            "[[792  12   9   9   5   1   4   1 136  31]\n",
            " [  4 912   0   0   0   0   1   0  12  71]\n",
            " [ 51   2 707  62  42  20  70  22  10  14]\n",
            " [ 22   5  11 789  20  56  49  17  11  20]\n",
            " [  9   2  14  52 841  17  26  23   9   7]\n",
            " [ 12   1   7 138  20 766  16  28   1  11]\n",
            " [  4   1   6  32   8   3 935   3   2   6]\n",
            " [ 11   2   3  24  23  16   3 902   4  12]\n",
            " [  8   8   0   3   0   0   4   3 968   6]\n",
            " [  1  16   0   2   0   0   3   0  20 958]]\n",
            "\n",
            "==> Best [Top1: 86.750   Top5: 99.330   Sparsity:0.00   Params: 301760 on epoch: 37]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [39][  100/  500]    Overall Loss 0.256237    Objective Loss 0.256237                                        LR 0.010000    Time 0.063902    \n",
            "Epoch: [39][  200/  500]    Overall Loss 0.261727    Objective Loss 0.261727                                        LR 0.010000    Time 0.051410    \n",
            "Epoch: [39][  300/  500]    Overall Loss 0.265343    Objective Loss 0.265343                                        LR 0.010000    Time 0.046308    \n",
            "Epoch: [39][  400/  500]    Overall Loss 0.267762    Objective Loss 0.267762                                        LR 0.010000    Time 0.044474    \n",
            "Epoch: [39][  500/  500]    Overall Loss 0.271395    Objective Loss 0.271395    Top1 91.000000    Top5 100.000000    LR 0.010000    Time 0.046735    \n",
            "--- validate (epoch=39)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [39][  100/  100]    Loss 0.434385    Top1 86.260000    Top5 99.410000    \n",
            "==> Top1: 86.260    Top5: 99.410    Loss: 0.434\n",
            "\n",
            "==> Confusion:\n",
            "[[884   7  23   7   6   4   0  19  37  13]\n",
            " [  4 937   0   0   3   2   0   3  10  41]\n",
            " [ 30   1 828  15  34  36  12  32   5   7]\n",
            " [ 18   1  52 636  53 154  14  58   3  11]\n",
            " [  2   0  44  17 844  36   5  49   3   0]\n",
            " [  3   0  15  49  19 858   1  49   3   3]\n",
            " [  8   2  60  51  22  20 816  14   2   5]\n",
            " [  4   0   7   7  10  18   0 952   0   2]\n",
            " [ 30   6   1   6   3   2   0   5 930  17]\n",
            " [ 15  19   3   3   0   3   1   3  12 941]]\n",
            "\n",
            "==> Best [Top1: 86.750   Top5: 99.330   Sparsity:0.00   Params: 301760 on epoch: 37]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [40][  100/  500]    Overall Loss 0.253452    Objective Loss 0.253452                                        LR 0.010000    Time 0.040755    \n",
            "Epoch: [40][  200/  500]    Overall Loss 0.264369    Objective Loss 0.264369                                        LR 0.010000    Time 0.038560    \n",
            "Epoch: [40][  300/  500]    Overall Loss 0.267163    Objective Loss 0.267163                                        LR 0.010000    Time 0.044831    \n",
            "Epoch: [40][  400/  500]    Overall Loss 0.265592    Objective Loss 0.265592                                        LR 0.010000    Time 0.044278    \n",
            "Epoch: [40][  500/  500]    Overall Loss 0.268206    Objective Loss 0.268206    Top1 89.000000    Top5 100.000000    LR 0.010000    Time 0.042691    \n",
            "--- validate (epoch=40)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [40][  100/  100]    Loss 0.457169    Top1 86.250000    Top5 99.450000    \n",
            "==> Top1: 86.250    Top5: 99.450    Loss: 0.457\n",
            "\n",
            "==> Confusion:\n",
            "[[821   1  29  21  10   3   0   8  93  14]\n",
            " [  5 905   0   2   1   3   1   2  43  38]\n",
            " [ 25   0 790  39  54  41  21  14   9   7]\n",
            " [  8   0  29 725  36 147  28  11   9   7]\n",
            " [  4   2  18  29 880  23  10  30   4   0]\n",
            " [  3   0   4  70  21 876   1  20   4   1]\n",
            " [  5   2  32  63  23  14 855   3   1   2]\n",
            " [  4   0  14  17  26  31   1 904   2   1]\n",
            " [ 13   3   2   5   1   1   4   1 966   4]\n",
            " [  9  23   4   6   0   2   2   3  48 903]]\n",
            "\n",
            "==> Best [Top1: 86.750   Top5: 99.330   Sparsity:0.00   Params: 301760 on epoch: 37]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [41][  100/  500]    Overall Loss 0.244523    Objective Loss 0.244523                                        LR 0.010000    Time 0.061484    \n",
            "Epoch: [41][  200/  500]    Overall Loss 0.255251    Objective Loss 0.255251                                        LR 0.010000    Time 0.056833    \n",
            "Epoch: [41][  300/  500]    Overall Loss 0.252474    Objective Loss 0.252474                                        LR 0.010000    Time 0.057031    \n",
            "Epoch: [41][  400/  500]    Overall Loss 0.255688    Objective Loss 0.255688                                        LR 0.010000    Time 0.054820    \n",
            "Epoch: [41][  500/  500]    Overall Loss 0.258925    Objective Loss 0.258925    Top1 88.500000    Top5 100.000000    LR 0.010000    Time 0.054646    \n",
            "--- validate (epoch=41)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [41][  100/  100]    Loss 0.419705    Top1 86.980000    Top5 99.540000    \n",
            "==> Top1: 86.980    Top5: 99.540    Loss: 0.420\n",
            "\n",
            "==> Confusion:\n",
            "[[922   7  13  10   4   0   8   2  23  11]\n",
            " [  6 967   0   0   1   1   1   0   4  20]\n",
            " [ 40   1 799  39  19  23  56  15   4   4]\n",
            " [ 18   6  25 760  16 106  38  16   5  10]\n",
            " [  9   1  39  50 762  45  45  46   2   1]\n",
            " [  8   2  24  95  11 825   8  25   0   2]\n",
            " [  5   1  12  37   2   9 929   4   0   1]\n",
            " [ 11   5  13  20   5  25   1 914   0   6]\n",
            " [ 40  18   1   7   2   3   4   3 910  12]\n",
            " [ 15  55   1   3   1   2   3   1   9 910]]\n",
            "\n",
            "==> Best [Top1: 86.980   Top5: 99.540   Sparsity:0.00   Params: 301760 on epoch: 41]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [42][  100/  500]    Overall Loss 0.247305    Objective Loss 0.247305                                        LR 0.010000    Time 0.042958    \n",
            "Epoch: [42][  200/  500]    Overall Loss 0.252235    Objective Loss 0.252235                                        LR 0.010000    Time 0.043666    \n",
            "Epoch: [42][  300/  500]    Overall Loss 0.252569    Objective Loss 0.252569                                        LR 0.010000    Time 0.047919    \n",
            "Epoch: [42][  400/  500]    Overall Loss 0.256190    Objective Loss 0.256190                                        LR 0.010000    Time 0.045322    \n",
            "Epoch: [42][  500/  500]    Overall Loss 0.258741    Objective Loss 0.258741    Top1 91.000000    Top5 100.000000    LR 0.010000    Time 0.043475    \n",
            "--- validate (epoch=42)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [42][  100/  100]    Loss 0.406299    Top1 87.090000    Top5 99.530000    \n",
            "==> Top1: 87.090    Top5: 99.530    Loss: 0.406\n",
            "\n",
            "==> Confusion:\n",
            "[[906  11  16  10   8   1   5   2  33   8]\n",
            " [  5 958   0   0   1   0   2   1   8  25]\n",
            " [ 45   1 817  35  29  14  40  12   5   2]\n",
            " [ 20   1  41 774  27  72  39  14   4   8]\n",
            " [  9   1  35  41 860  15  20  15   4   0]\n",
            " [  9   0  24 140  25 759   8  34   0   1]\n",
            " [  7   2  33  46   8   4 897   1   1   1]\n",
            " [ 10   1  19  27  19  18   1 904   0   1]\n",
            " [ 34  14   3  10   1   1   2   1 927   7]\n",
            " [ 25  40   2   4   2   0   2   4  14 907]]\n",
            "\n",
            "==> Best [Top1: 87.090   Top5: 99.530   Sparsity:0.00   Params: 301760 on epoch: 42]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [43][  100/  500]    Overall Loss 0.244298    Objective Loss 0.244298                                        LR 0.010000    Time 0.066879    \n",
            "Epoch: [43][  200/  500]    Overall Loss 0.248363    Objective Loss 0.248363                                        LR 0.010000    Time 0.052224    \n",
            "Epoch: [43][  300/  500]    Overall Loss 0.251414    Objective Loss 0.251414                                        LR 0.010000    Time 0.046820    \n",
            "Epoch: [43][  400/  500]    Overall Loss 0.255462    Objective Loss 0.255462                                        LR 0.010000    Time 0.045011    \n",
            "Epoch: [43][  500/  500]    Overall Loss 0.254985    Objective Loss 0.254985    Top1 91.000000    Top5 100.000000    LR 0.010000    Time 0.047541    \n",
            "--- validate (epoch=43)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [43][  100/  100]    Loss 0.514241    Top1 85.490000    Top5 99.170000    \n",
            "==> Top1: 85.490    Top5: 99.170    Loss: 0.514\n",
            "\n",
            "==> Confusion:\n",
            "[[931  18   3   3   2   0   0   0  19  24]\n",
            " [  4 977   0   0   0   0   0   0   3  16]\n",
            " [ 96   6 783  19  25  17  25   7   6  16]\n",
            " [ 48  19  46 707  33  64  27  13   9  34]\n",
            " [ 25   4  30  22 876   9  22   6   2   4]\n",
            " [ 23  11  25 114  34 751   8  19   2  13]\n",
            " [ 20   8  35  33  12   3 869   2   2  16]\n",
            " [ 31  15  11  16  37  14   2 855   2  17]\n",
            " [ 43  31   0   1   0   0   1   0 902  22]\n",
            " [  9  84   0   0   1   0   0   0   8 898]]\n",
            "\n",
            "==> Best [Top1: 87.090   Top5: 99.530   Sparsity:0.00   Params: 301760 on epoch: 42]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [44][  100/  500]    Overall Loss 0.243873    Objective Loss 0.243873                                        LR 0.010000    Time 0.041149    \n",
            "Epoch: [44][  200/  500]    Overall Loss 0.243720    Objective Loss 0.243720                                        LR 0.010000    Time 0.039067    \n",
            "Epoch: [44][  300/  500]    Overall Loss 0.245492    Objective Loss 0.245492                                        LR 0.010000    Time 0.045045    \n",
            "Epoch: [44][  400/  500]    Overall Loss 0.249932    Objective Loss 0.249932                                        LR 0.010000    Time 0.044131    \n",
            "Epoch: [44][  500/  500]    Overall Loss 0.251920    Objective Loss 0.251920    Top1 89.000000    Top5 100.000000    LR 0.010000    Time 0.042436    \n",
            "--- validate (epoch=44)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [44][  100/  100]    Loss 0.420684    Top1 86.800000    Top5 99.520000    \n",
            "==> Top1: 86.800    Top5: 99.520    Loss: 0.421\n",
            "\n",
            "==> Confusion:\n",
            "[[872   9  17  22   3   0   1   2  64  10]\n",
            " [  6 944   0   2   0   0   0   0  24  24]\n",
            " [ 43   1 728  66  64  29  38  16   9   6]\n",
            " [ 14   2  13 818  37  67  34   7   2   6]\n",
            " [  4   1  13  43 889  15  20  10   4   1]\n",
            " [  7   0   5 140  31 789   6  20   0   2]\n",
            " [  6   2  21  46  15   4 901   1   3   1]\n",
            " [ 12   2   9  30  49  22   3 867   1   5]\n",
            " [ 15   6   2   8   2   1   6   0 953   7]\n",
            " [ 10  41   2   8   1   0   1   1  17 919]]\n",
            "\n",
            "==> Best [Top1: 87.090   Top5: 99.530   Sparsity:0.00   Params: 301760 on epoch: 42]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [45][  100/  500]    Overall Loss 0.239358    Objective Loss 0.239358                                        LR 0.010000    Time 0.059158    \n",
            "Epoch: [45][  200/  500]    Overall Loss 0.244906    Objective Loss 0.244906                                        LR 0.010000    Time 0.052908    \n",
            "Epoch: [45][  300/  500]    Overall Loss 0.246052    Objective Loss 0.246052                                        LR 0.010000    Time 0.047461    \n",
            "Epoch: [45][  400/  500]    Overall Loss 0.244441    Objective Loss 0.244441                                        LR 0.010000    Time 0.044674    \n",
            "Epoch: [45][  500/  500]    Overall Loss 0.247576    Objective Loss 0.247576    Top1 85.500000    Top5 99.500000    LR 0.010000    Time 0.045565    \n",
            "--- validate (epoch=45)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [45][  100/  100]    Loss 0.471945    Top1 85.730000    Top5 99.420000    \n",
            "==> Top1: 85.730    Top5: 99.420    Loss: 0.472\n",
            "\n",
            "==> Confusion:\n",
            "[[891  17  16   2   8   2   2   1  31  30]\n",
            " [  4 971   0   0   0   1   0   0   4  20]\n",
            " [ 59   4 802  22  37  24  16  14  11  11]\n",
            " [ 26  13  72 671  30 113  31  10  14  20]\n",
            " [ 13   3  33  23 866  21  11  24   5   1]\n",
            " [ 13   1  35  78  19 819   9  17   3   6]\n",
            " [ 16   7  48  31  11   8 862   2   9   6]\n",
            " [ 20   7  21  15  23  39   1 857   3  14]\n",
            " [ 32  26   1   3   1   0   0   0 915  22]\n",
            " [ 10  62   1   0   0   1   0   0   7 919]]\n",
            "\n",
            "==> Best [Top1: 87.090   Top5: 99.530   Sparsity:0.00   Params: 301760 on epoch: 42]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [46][  100/  500]    Overall Loss 0.242874    Objective Loss 0.242874                                        LR 0.010000    Time 0.040271    \n",
            "Epoch: [46][  200/  500]    Overall Loss 0.239265    Objective Loss 0.239265                                        LR 0.010000    Time 0.037621    \n",
            "Epoch: [46][  300/  500]    Overall Loss 0.244356    Objective Loss 0.244356                                        LR 0.010000    Time 0.043034    \n",
            "Epoch: [46][  400/  500]    Overall Loss 0.247577    Objective Loss 0.247577                                        LR 0.010000    Time 0.043693    \n",
            "Epoch: [46][  500/  500]    Overall Loss 0.248468    Objective Loss 0.248468    Top1 93.000000    Top5 100.000000    LR 0.010000    Time 0.042113    \n",
            "--- validate (epoch=46)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [46][  100/  100]    Loss 0.475113    Top1 85.550000    Top5 99.420000    \n",
            "==> Top1: 85.550    Top5: 99.420    Loss: 0.475\n",
            "\n",
            "==> Confusion:\n",
            "[[802  16  35   5  10   8  14   8  76  26]\n",
            " [  3 939   1   0   1   1   1   1   9  44]\n",
            " [ 21   2 783  15  40  43  69  13   6   8]\n",
            " [  7   2  37 527  28 234 110  29  11  15]\n",
            " [  5   2  20  20 841  38  40  29   5   0]\n",
            " [  5   0  13  25  20 891  14  26   2   4]\n",
            " [  4   2   5   7   4  15 959   3   1   0]\n",
            " [  5   3   5   4  17  38   6 915   1   6]\n",
            " [ 13   8   2   1   1   4   5   1 952  13]\n",
            " [  3  30   0   1   0   3   5   0  12 946]]\n",
            "\n",
            "==> Best [Top1: 87.090   Top5: 99.530   Sparsity:0.00   Params: 301760 on epoch: 42]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [47][  100/  500]    Overall Loss 0.216881    Objective Loss 0.216881                                        LR 0.010000    Time 0.054214    \n",
            "Epoch: [47][  200/  500]    Overall Loss 0.227425    Objective Loss 0.227425                                        LR 0.010000    Time 0.050498    \n",
            "Epoch: [47][  300/  500]    Overall Loss 0.233652    Objective Loss 0.233652                                        LR 0.010000    Time 0.049668    \n",
            "Epoch: [47][  400/  500]    Overall Loss 0.235243    Objective Loss 0.235243                                        LR 0.010000    Time 0.052922    \n",
            "Epoch: [47][  500/  500]    Overall Loss 0.237795    Objective Loss 0.237795    Top1 91.000000    Top5 100.000000    LR 0.010000    Time 0.053897    \n",
            "--- validate (epoch=47)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [47][  100/  100]    Loss 0.527986    Top1 84.460000    Top5 99.310000    \n",
            "==> Top1: 84.460    Top5: 99.310    Loss: 0.528\n",
            "\n",
            "==> Confusion:\n",
            "[[722  11  53   9  32   9   8  24 121  11]\n",
            " [  4 950   2   1   0   2   0   6  20  15]\n",
            " [ 13   0 802  17  65  26  22  50   3   2]\n",
            " [  6   0  39 624  72 172  15  68   2   2]\n",
            " [  1   1  13  10 912  22   4  36   1   0]\n",
            " [  1   0  11  45  31 849   2  60   1   0]\n",
            " [  3   3  30  51  49  25 808  27   3   1]\n",
            " [  3   0   4   6  32  12   0 943   0   0]\n",
            " [ 13   7   3   6   4   4   1   6 953   3]\n",
            " [  8  53   4   3   1   3   0  15  30 883]]\n",
            "\n",
            "==> Best [Top1: 87.090   Top5: 99.530   Sparsity:0.00   Params: 301760 on epoch: 42]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [48][  100/  500]    Overall Loss 0.224979    Objective Loss 0.224979                                        LR 0.010000    Time 0.043325    \n",
            "Epoch: [48][  200/  500]    Overall Loss 0.234303    Objective Loss 0.234303                                        LR 0.010000    Time 0.041423    \n",
            "Epoch: [48][  300/  500]    Overall Loss 0.237041    Objective Loss 0.237041                                        LR 0.010000    Time 0.046767    \n",
            "Epoch: [48][  400/  500]    Overall Loss 0.237569    Objective Loss 0.237569                                        LR 0.010000    Time 0.044117    \n",
            "Epoch: [48][  500/  500]    Overall Loss 0.240218    Objective Loss 0.240218    Top1 93.000000    Top5 99.500000    LR 0.010000    Time 0.042385    \n",
            "--- validate (epoch=48)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [48][  100/  100]    Loss 0.424684    Top1 86.580000    Top5 99.510000    \n",
            "==> Top1: 86.580    Top5: 99.510    Loss: 0.425\n",
            "\n",
            "==> Confusion:\n",
            "[[865   2  52   8  16   5   9   5  32   6]\n",
            " [ 13 937   2   2   2   3   0   1  15  25]\n",
            " [ 16   0 842  32  45  28  22   7   4   4]\n",
            " [ 11   0  69 729  35 127  20   6   1   2]\n",
            " [  5   0  31  42 866  28   9  17   2   0]\n",
            " [  4   1  26  97  22 834   2  12   0   2]\n",
            " [  1   0  38  58  23  23 853   2   1   1]\n",
            " [  6   1  22  30  27  25   0 887   0   2]\n",
            " [ 34   7   5   9   2   5   4   3 925   6]\n",
            " [ 17  35   4   6   0   4   2   0  12 920]]\n",
            "\n",
            "==> Best [Top1: 87.090   Top5: 99.530   Sparsity:0.00   Params: 301760 on epoch: 42]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "\n",
            "\n",
            "Training epoch: 50000 samples (100 per mini-batch)\n",
            "Epoch: [49][  100/  500]    Overall Loss 0.227587    Objective Loss 0.227587                                        LR 0.010000    Time 0.063122    \n",
            "Epoch: [49][  200/  500]    Overall Loss 0.227993    Objective Loss 0.227993                                        LR 0.010000    Time 0.053159    \n",
            "Epoch: [49][  300/  500]    Overall Loss 0.228759    Objective Loss 0.228759                                        LR 0.010000    Time 0.047412    \n",
            "Epoch: [49][  400/  500]    Overall Loss 0.231047    Objective Loss 0.231047                                        LR 0.010000    Time 0.044655    \n",
            "Epoch: [49][  500/  500]    Overall Loss 0.230327    Objective Loss 0.230327    Top1 93.000000    Top5 100.000000    LR 0.010000    Time 0.046584    \n",
            "--- validate (epoch=49)-----------\n",
            "10000 samples (100 per mini-batch)\n",
            "Epoch: [49][  100/  100]    Loss 0.431733    Top1 86.930000    Top5 99.470000    \n",
            "==> Top1: 86.930    Top5: 99.470    Loss: 0.432\n",
            "\n",
            "==> Confusion:\n",
            "[[868   9  57  15   5   0   2   1  31  12]\n",
            " [  7 949   0   1   0   2   0   0  15  26]\n",
            " [ 14   0 868  30  23  18  30   7   5   5]\n",
            " [ 16   3  45 735  26 118  27   8  13   9]\n",
            " [ 13   1  40  36 847   9  29  21   3   1]\n",
            " [ 10   2  25  87  23 827   8  13   2   3]\n",
            " [  4   2  28  41   7   6 906   2   2   2]\n",
            " [ 15   2  32  30  27  36   2 838   8  10]\n",
            " [ 31   6   4   5   0   0   3   0 943   8]\n",
            " [ 13  45   4   6   0   1   1   0  18 912]]\n",
            "\n",
            "==> Best [Top1: 87.090   Top5: 99.530   Sparsity:0.00   Params: 301760 on epoch: 42]\n",
            "Saving checkpoint to: logs/2024.05.11-050006/checkpoint.pth.tar\n",
            "--- test ---------------------\n",
            "10000 samples (100 per mini-batch)\n",
            "Test: [  100/  100]    Loss 0.431733    Top1 86.930000    Top5 99.470000    \n",
            "==> Top1: 86.930    Top5: 99.470    Loss: 0.432\n",
            "\n",
            "==> Confusion:\n",
            "[[868   9  57  15   5   0   2   1  31  12]\n",
            " [  7 949   0   1   0   2   0   0  15  26]\n",
            " [ 14   0 868  30  23  18  30   7   5   5]\n",
            " [ 16   3  45 735  26 118  27   8  13   9]\n",
            " [ 13   1  40  36 847   9  29  21   3   1]\n",
            " [ 10   2  25  87  23 827   8  13   2   3]\n",
            " [  4   2  28  41   7   6 906   2   2   2]\n",
            " [ 15   2  32  30  27  36   2 838   8  10]\n",
            " [ 31   6   4   5   0   0   3   0 943   8]\n",
            " [ 13  45   4   6   0   1   1   0  18 912]]\n",
            "\n",
            "\n",
            "Log file for this run: /content/ai8x-training/logs/2024.05.11-050006/2024.05.11-050006.log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/ai8x-training/logs/2024.05.11-050006/2024.05.11-050006.log"
      ],
      "metadata": {
        "id": "NrYhlqQJ-H1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Synthesis"
      ],
      "metadata": {
        "id": "0w2U8sgJ-Sv_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1dUGhmUVzYU9",
        "outputId": "32f24e3b-bf7c-4db9-9a36-0562d6c9672a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai8x-synthesis'...\n",
            "remote: Enumerating objects: 5541, done.\u001b[K\n",
            "remote: Counting objects: 100% (968/968), done.\u001b[K\n",
            "remote: Compressing objects: 100% (424/424), done.\u001b[K\n",
            "remote: Total 5541 (delta 678), reused 779 (delta 532), pack-reused 4573\u001b[K\n",
            "Receiving objects: 100% (5541/5541), 585.65 MiB | 30.82 MiB/s, done.\n",
            "Resolving deltas: 100% (3737/3737), done.\n",
            "Updating files: 100% (1311/1311), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -fr ai8x-synthesis/; git clone --recursive https://github.com/analogdevicesinc/ai8x-synthesis.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OyLyeVUzyRW",
        "outputId": "7067f84f-c90b-4917-8f00-b330a57e5570"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ai8x-synthesis/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyaL2z5H16MI",
        "outputId": "ca2fe3bd-728e-4a19-e983-5cb4012891bc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ai8x-synthesis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!source .venv/bin/activate;pip3 install -U pip wheel setuptools; pip3 install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "56jsXypp2Eb0",
        "outputId": "eaf54f17-c4b1-4e07-955b-1d8afbe3e40a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: .venv/bin/activate: No such file or directory\n",
            "Requirement already satisfied: pip in /root/.local/lib/python3.8/site-packages (24.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (0.43.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (69.5.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy<1.23,>=1.22 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: PyYAML>=5.1.1 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 2)) (5.4.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 3)) (1.16.0)\n",
            "Collecting scipy>=1.3.0 (from -r requirements.txt (line 4))\n",
            "  Using cached scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting torch==1.8.1 (from -r requirements.txt (line 5))\n",
            "  Using cached torch-1.8.1-cp38-cp38-manylinux1_x86_64.whl.metadata (23 kB)\n",
            "Collecting pytest~=4.6.4 (from -r requirements.txt (line 6))\n",
            "  Using cached pytest-4.6.11-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting onnx>=1.9.0 (from -r requirements.txt (line 7))\n",
            "  Using cached onnx-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting GitPython>=3.1.18 (from -r requirements.txt (line 8))\n",
            "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting PyGithub>=1.55 (from -r requirements.txt (line 9))\n",
            "  Using cached PyGithub-2.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting rich>=11.2.0 (from -r requirements.txt (line 10))\n",
            "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: protobuf<4.0,>=3.20.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (3.20.3)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 12)) (3.4.1)\n",
            "Collecting matplotlib>=3.6.0 (from -r requirements.txt (line 13))\n",
            "  Using cached matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1->-r requirements.txt (line 5)) (4.11.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from pytest~=4.6.4->-r requirements.txt (line 6)) (1.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from pytest~=4.6.4->-r requirements.txt (line 6)) (24.0)\n",
            "Collecting attrs>=17.4.0 (from pytest~=4.6.4->-r requirements.txt (line 6))\n",
            "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting atomicwrites>=1.0 (from pytest~=4.6.4->-r requirements.txt (line 6))\n",
            "  Using cached atomicwrites-1.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.8/dist-packages (from pytest~=4.6.4->-r requirements.txt (line 6)) (0.13.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from pytest~=4.6.4->-r requirements.txt (line 6)) (0.2.13)\n",
            "Collecting more-itertools>=4.0.0 (from pytest~=4.6.4->-r requirements.txt (line 6))\n",
            "  Using cached more_itertools-10.2.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython>=3.1.18->-r requirements.txt (line 8))\n",
            "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting pynacl>=1.4.0 (from PyGithub>=1.55->-r requirements.txt (line 9))\n",
            "  Using cached PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Collecting requests>=2.14.0 (from PyGithub>=1.55->-r requirements.txt (line 9))\n",
            "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub>=1.55->-r requirements.txt (line 9)) (2.8.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.8/dist-packages (from PyGithub>=1.55->-r requirements.txt (line 9)) (2.2.1)\n",
            "Collecting Deprecated (from PyGithub>=1.55->-r requirements.txt (line 9))\n",
            "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=11.2.0->-r requirements.txt (line 10))\n",
            "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from rich>=11.2.0->-r requirements.txt (line 10)) (2.18.0)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib>=3.6.0->-r requirements.txt (line 13))\n",
            "  Using cached contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib>=3.6.0->-r requirements.txt (line 13))\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib>=3.6.0->-r requirements.txt (line 13))\n",
            "  Using cached fonttools-4.51.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
            "Collecting kiwisolver>=1.0.1 (from matplotlib>=3.6.0->-r requirements.txt (line 13))\n",
            "  Using cached kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.6.0->-r requirements.txt (line 13)) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.6.0->-r requirements.txt (line 13)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.6.0->-r requirements.txt (line 13)) (2.9.0.post0)\n",
            "Collecting importlib-resources>=3.2.0 (from matplotlib>=3.6.0->-r requirements.txt (line 13))\n",
            "  Using cached importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.18->-r requirements.txt (line 8)) (5.0.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.6.0->-r requirements.txt (line 13)) (3.18.1)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.2.0->-r requirements.txt (line 10))\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting cryptography>=3.4.0 (from pyjwt[crypto]>=2.4.0->PyGithub>=1.55->-r requirements.txt (line 9))\n",
            "  Using cached cryptography-42.0.7-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting cffi>=1.4.1 (from pynacl>=1.4.0->PyGithub>=1.55->-r requirements.txt (line 9))\n",
            "  Using cached cffi-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests>=2.14.0->PyGithub>=1.55->-r requirements.txt (line 9))\n",
            "  Using cached charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting idna<4,>=2.5 (from requests>=2.14.0->PyGithub>=1.55->-r requirements.txt (line 9))\n",
            "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests>=2.14.0->PyGithub>=1.55->-r requirements.txt (line 9))\n",
            "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from Deprecated->PyGithub>=1.55->-r requirements.txt (line 9)) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub>=1.55->-r requirements.txt (line 9)) (2.22)\n",
            "Using cached torch-1.8.1-cp38-cp38-manylinux1_x86_64.whl (804.1 MB)\n",
            "Using cached scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "Using cached pytest-4.6.11-py2.py3-none-any.whl (231 kB)\n",
            "Using cached onnx-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "Using cached PyGithub-2.3.0-py3-none-any.whl (354 kB)\n",
            "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
            "Using cached matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
            "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "Using cached contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached fonttools-4.51.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "Using cached importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
            "Using cached kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Using cached more_itertools-10.2.0-py3-none-any.whl (57 kB)\n",
            "Using cached PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
            "Using cached cffi-1.16.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (444 kB)\n",
            "Using cached charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "Using cached cryptography-42.0.7-cp37-abi3-manylinux_2_28_x86_64.whl (3.8 MB)\n",
            "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
            "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: torch, scipy, onnx, more-itertools, mdurl, kiwisolver, importlib-resources, idna, gitdb, fonttools, Deprecated, cycler, contourpy, charset-normalizer, cffi, certifi, attrs, atomicwrites, requests, pytest, pynacl, matplotlib, markdown-it-py, GitPython, cryptography, rich, PyGithub\n",
            "Successfully installed Deprecated-1.2.14 GitPython-3.1.43 PyGithub-2.3.0 atomicwrites-1.4.1 attrs-23.2.0 certifi-2024.2.2 cffi-1.16.0 charset-normalizer-3.3.2 contourpy-1.1.1 cryptography-42.0.7 cycler-0.12.1 fonttools-4.51.0 gitdb-4.0.11 idna-3.7 importlib-resources-6.4.0 kiwisolver-1.4.5 markdown-it-py-3.0.0 matplotlib-3.7.5 mdurl-0.1.2 more-itertools-10.2.0 onnx-1.16.0 pynacl-1.5.0 pytest-4.6.11 requests-2.31.0 rich-13.7.1 scipy-1.10.1 torch-1.8.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 ai8xize.py --test-dir sdk/Examples/MAX78000/CNN --prefix cifar-10 --checkpoint-file trained/ai85-cifar10-qat8-q.pth.tar --config-file networks/cifar10-nas.yaml --sample-input tests/sample_cifar-10.npy --softmax --device MAX78000 --timer 0 --display-checkpoint --verbose\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSMI1PZUzyOQ",
        "outputId": "6507859e-7374-4d53-9ec3-d45594128708"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Following Github server redirection from /repos/MaximIntegratedAI/ai8x-synthesis to /repositories/265377030\n",
            "Configuring device: MAX78000\n",
            "Reading networks/cifar10-nas.yaml to configure network...\n",
            "\u001b[33mWARNING:\u001b[0m Cannot run \"yamllint\" linter to check networks/cifar10-nas.yaml\n",
            "Reading trained/ai85-cifar10-qat8-q.pth.tar to configure network weights...\n",
            "Checkpoint for epoch 243, model ai85nascifarnet - weight and bias data:\n",
            " InCh OutCh  Weights         Quant Shift  Min  Max    Size Key                                       Bias       Quant  Min  Max Size Key\n",
            "    3    64  (192, 3, 3)         8     0 -121   99    1728 conv1_1.op.weight                         (64,)          8  -13   14   64 conv1_1.op.bias          \n",
            "   64    32  (2048, 1, 1)        8    -1 -128  127    2048 conv1_2.op.weight                         (32,)          8  -64   77   32 conv1_2.op.bias          \n",
            "   32    64  (2048, 3, 3)        8    -1 -128  110   18432 conv1_3.op.weight                         (64,)          8  -89  116   64 conv1_3.op.bias          \n",
            "   64    32  (2048, 3, 3)        8    -3 -128  127   18432 conv2_1.op.weight                         (32,)          8 -128  127   32 conv2_1.op.bias          \n",
            "   32    64  (2048, 1, 1)        8     0 -124  126    2048 conv2_2.op.weight                         (64,)          8  -50   55   64 conv2_2.op.bias          \n",
            "   64   128  (8192, 3, 3)        8    -3 -128  127   73728 conv3_1.op.weight                         (128,)         8 -128  127  128 conv3_1.op.bias          \n",
            "  128   128  (16384, 1, 1)       8    -1 -128  127   16384 conv3_2.op.weight                         (128,)         8 -101  127  128 conv3_2.op.bias          \n",
            "  128    64  (8192, 3, 3)        8    -3 -128  127   73728 conv4_1.op.weight                         (64,)          8 -128  127   64 conv4_1.op.bias          \n",
            "   64   128  (8192, 3, 3)        8    -2 -128  127   73728 conv4_2.op.weight                         (128,)         8 -128  103  128 conv4_2.op.bias          \n",
            "  128   128  (16384, 1, 1)       8    -1 -102  123   16384 conv5_1.op.weight                         (128,)         8 -128  127  128 conv5_1.op.bias          \n",
            "  512    10  (1, 10, 512)        8     1 -128   94    5120 fc.op.weight                              (10,)          8   -7    7   10 fc.op.bias               \n",
            "TOTAL: 11 parameter layers, 302,602 parameters, 302,602 bytes\n",
            "Configuring data set: CIFAR10.\n",
            "cifar-10...\n",
            "\u001b[2KArranging weights... \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  9%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 18%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 18%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 27%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 36%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 45%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 45%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 45%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 55%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 55%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 55%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 55%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[35m 64%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[35m 73%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[35m 73%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[35m 82%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[35m 82%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[35m 82%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[35m 82%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m 91%\u001b[0m\n",
            "\u001b[2K\u001b[1A\u001b[2KArranging weights... \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m\n",
            "\u001b[2KStoring weights...   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m\n",
            "\u001b[2KCreating network...  \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generated Code:"
      ],
      "metadata": {
        "id": "zb7lzsfQChgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -latrh sdk/Examples/MAX78000/CNN/cifar-10/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUgAyW7s4mLN",
        "outputId": "b95c6504-ba69-4942-9187-6475d643e635"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 7.6M\n",
            "drwxr-xr-x 3 root root 4.0K May 11 04:30 ..\n",
            "-rw-r--r-- 1 root root  322 May 11 04:30 sampleoutput.h\n",
            "-rw-r--r-- 1 root root  13K May 11 04:30 sampledata.h\n",
            "-rw-r--r-- 1 root root 6.1K May 11 04:30 main.c\n",
            "-rw-r--r-- 1 root root  46K May 11 04:30 cnn.c\n",
            "-rw-r--r-- 1 root root 938K May 11 04:30 weights.h\n",
            "-rw-r--r-- 1 root root  14K May 11 04:30 Makefile\n",
            "-rw-r--r-- 1 root root  361 May 11 04:30 project.mk\n",
            "-rw-r--r-- 1 root root 5.6K May 11 04:30 cifar-10.launch\n",
            "-rw-r--r-- 1 root root  760 May 11 04:30 .project\n",
            "-rw-r--r-- 1 root root 7.0K May 11 04:30 .cproject\n",
            "drwxr-xr-x 2 root root 4.0K May 11 04:30 .settings\n",
            "drwxr-xr-x 2 root root 4.0K May 11 04:30 .vscode\n",
            "-rw-r--r-- 1 root root 4.2K May 11 04:30 softmax.c\n",
            "-rw-r--r-- 1 root root 3.7K May 11 04:30 cnn.h\n",
            "drwxr-xr-x 4 root root 4.0K May 11 04:30 .\n",
            "-rw-r--r-- 1 root root 6.5M May 11 04:30 log.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r cifar-10.zip sdk/Examples/MAX78000/CNN/cifar-10/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OvSiJVUd8jer",
        "outputId": "37e4b819-4eba-436f-ec49-6f573aa764eb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/ (stored 0%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/weights.h (deflated 58%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/.settings/ (stored 0%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/.settings/org.eclipse.cdt.core.prefs (deflated 82%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/.settings/language.settings.xml (deflated 57%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/.settings/org.eclipse.cdt.codan.core.prefs (deflated 90%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/sampledata.h (deflated 66%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/.vscode/ (stored 0%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/.vscode/c_cpp_properties.json (deflated 82%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/.vscode/tasks.json (deflated 84%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/.vscode/launch.json (deflated 82%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/.vscode/README.md (deflated 55%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/.vscode/flash.gdb (deflated 63%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/.vscode/settings.json (deflated 79%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/.cproject (deflated 75%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/sampleoutput.h (deflated 46%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/main.c (deflated 56%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/softmax.c (deflated 64%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/log.txt (deflated 74%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/cifar-10.launch (deflated 80%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/.project (deflated 65%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/Makefile (deflated 65%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/cnn.h (deflated 61%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/project.mk (deflated 41%)\n",
            "  adding: sdk/Examples/MAX78000/CNN/cifar-10/cnn.c (deflated 89%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MZ42oQZ_9QGJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}